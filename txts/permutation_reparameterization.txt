Published as a conference paper at ICLR 2018

L EARN ING LATEN T P ERMU TAT ION S W I TH GUMB EL -
S INKHORN N ETWORK S

Gonzalo E. Mena ∗

Department of Statistics,
Columbia University

gem2131@columbia.edu

David Belanger

Google Brain

Scott Linderman

Department of Statistics,
Columbia University

Jasper Snoek

Google Brain

8
1
0
2

b

e

F

3
2

]

L

M

.

t

a

t

s

[

1
v
5
6
6
8
0

.

2
0
8
1

:

v

i

X

r

a

AB STRAC T

Permutations and matchings are core building blocks in a variety of latent vari-
able models, as they allow us to align, canonicalize, and sort data. Learning in
such models is difﬁcult, however, because exact marginalization over these com-
binatorial objects is intractable. In response, this paper introduces a collection
of new methods for end-to-end learning in such models that approximate discrete
maximum-weight matching using the continuous Sinkhorn operator. Sinkhorn
operator is attractive because it functions as a simple, easy-to-implement analog
of the softmax operator. With this, we can deﬁne the Gumbel-Sinkhorn method,
an extension of the Gumbel-Softmax method (Jang et al., 2016; Maddison et al.,
2016) to distributions over latent matchings. We demonstrate the effectiveness
of our method by outperforming competitive baselines on a range of qualitatively
different tasks: sorting numbers, solving jigsaw puzzles, and identifying neural
signals in worms.

1

IN TRODUC T ION

In principle, deep networks can learn arbitrarily sophisticated mappings from inputs to outputs.
However, in practice we must encode speciﬁc inductive biases in order to learn accurate models
from limit data. In a variety of recent research efforts, practitioners have provided models with
the ability to explicitly manipulate latent combinatorial objects such as stacks (Dyer et al., 2015;
Joulin & Mikolov, 2015), memory slots (Graves et al., 2014; Sukhbaatar et al., 2015), mathematical
expressions (Neelakantan et al., 2015), program traces (Gaunt et al., 2016; Boˇsnjak et al., 2017),
and ﬁrst order logic (Rockt ¨aschel & Riedel, 2017). Operations on these discrete objects can be
approximated using differentiable operations on continuous relaxations of the objects. As such,
these operations can be included as modules in neural network models that can be trained end-to-
end by gradient descent.
Matchings and permutations are a fundamental building block in a variety of applications, as they
can be used to align, canonicalize, and sort data. Prior work has developed learning algorithms
for supervised learning where the training data includes annotated matchings (Caetano et al., 2009;
Petterson et al., 2009; Tang et al., 2016). However, we would like to learn models with latent
matchings, where the matching is not provided to us as supervision. This is a common and relevant
setting. For example, Linderman et al. (2017) showed a problem from neuroscience involving the
identiﬁcation of neurons from the worm C. elegans can be cast as the inference of latent permutation
on a larger hierarchical structure.
Unfortunately, maximizing the marginal likelihood for problems with latent matchings is very chal-
lenging. Unlike for problems with categorical latent variables, we cannot obtain unbiased stochastic
gradients of the marginal likelihood using the score function estimator (Williams, 1992), as com-
puting the probability of a given matching requires computing an intractable partition function
for a structured distribution. Instead, we draw on recent work that obtains biased stochastic gra-
dients by relaxing the discrete latent variables into continuous random variables that support the
reparametrization trick (Jang et al., 2016; Maddison et al., 2016).

∗Work done while the author was at Google Brain.

1

 
 
 
 
 
 
Published as a conference paper at ICLR 2018

Our contributions are the following: ﬁrst, in Section 2 we present a theoretical result showing that
the non-differentiable parameterization of a permutation can be approximated in terms of a differ-
entiable relaxation, the so-called Sinkhorn operator. Based on this result, in Section 3 we introduce
Sinkhorn networks, which generalize the work of method of Adams & Zemel (2011) for predict-
ing rankings, and complements the concurrent work by Cruz et al. (2017), by focusing on more
fundamental aspects. Further, in Section 4 we introduce the Gumbel-Sinkhorn, an analog of the
Gumbel Softmax distribution (Jang et al., 2016; Maddison et al., 2016) for permutations. This en-
ables optimization of the marginal likelihood by the reparametrization trick. Finally, in Section 5
we demonstrate that our methods outperform strong neural network baselines on the tasks of sorting
numbers, solving jigsaw puzzles, and identifying neural signals from C. elegans worms.

2 TH E S INKHORN O PERATOR : AN ANALOG O F TH E SO F TMAX FOR
P ERMU TAT ION S

exp(xi /τ )/ (cid:80)

One sensible way to approximate a discrete category by continuous values is by us-
ing a temperature-dependent softmax function, component-wise deﬁned as softmaxτ (x)i =
j=1 exp(xj /τ ). For positive values of τ , softmaxτ (x)i is a point in the probabil-
ity simplex. Also, in the limit τ → 0, softmaxτ (x)i converges to a vertex of the simplex, a one-hot
vector corresponding to the largest xi
1 . This approximation is a key ingredient in the successful
implementations by Jang et al. (2016); Maddison et al. (2016), and here we extend it to permutations.
To do so, we ﬁrst state an analog of the normalization implemented by the softmax. This is achieved
through the Sinkhorn operator (or Sinkhorn normalization, or Sinkhorn balancing), which iteratively
normalizes rows and columns of a matrix. Speciﬁcally, following Adams & Zemel (2011), we deﬁne
the Sinkhorn operator S (X ) over an N dimensional square matrix X as:

(1)

(cid:0)Tr (S l−1 (X ))(cid:1) ,
S 0 (X ) = exp(X ),
S l (X ) = Tc
S (X ) = lim
l→∞ S l (X ).
where Tr (X ) = X (cid:11) (X 1N 1(cid:62)
N ), and Tc (X ) = X (cid:11) (1N 1(cid:62)

N X ) as the row and column-wise
normalization operators of a matrix, with (cid:11) denoting the element-wise division and 1N a column
vector of ones. Sinkhorn (1964) proved that S (X ) must belong to the Birkhoff polytope, the set of
doubly stochastic matrices, that we denote BN
2 .
Building on our analogy with categories, notice that choosing a category can always be cast as
the choice arg maxi xi is the one that maximizes the function (cid:104)x, v(cid:105)
a maximization problem:
(with v being a one-hot vector), i.e. the maximizing v∗ indexes the largest xi . Similarly, one
may parameterize the choice of a permutation P through a square matrix X , as the solution to
the linear assignment problem (Kuhn, 1955), with PN denoting the set of permutation matrices and
(cid:104)A, B (cid:105)F = trace(A(cid:62)B ) the (Frobenius) inner product of matrices:

M (X ) = arg max

(cid:104)P , X (cid:105)F .

P ∈PN
We call M (·) the matching operator, through which we parameterize the hard choice of a permu-
tation (see Figure 3a for an example). Our theoretical contribution is to show that M (X ) can be
obtained as the limit of S (X/τ ), meaning that one can approximate M (X ) ≈ S (X/τ ) with a small
τ . Theorem 1 summarizes our ﬁnding. We provide a rigorous proof in appendix A; brieﬂy, it is
Theorem 1. For a doubly-stochastic matrix P , deﬁne its entropy as h(P ) = − (cid:80)
based on showing that S (X/τ ) solves a certain entropy-regularized problem in Bn , which in the
limit converges to the matching problem in equation 2.
Then, one has,
P ∈BN

(cid:104)P , X (cid:105)F + τ h(P ).

i,j Pi,j log (Pi,j ).

S (X/τ ) = arg max

(2)

(3)

1With the exception of the degenerate case of ties.
2This theorem requires certain technical conditions which are trivially satisﬁed if X has positive entries,
motivating the use of the component-wise exponential exp(·) in the ﬁrst line of equation 1.

2

Published as a conference paper at ICLR 2018

Now, assume also the entries of X are drawn independently from a distribution that is absolutely
continuous with respect to the Lebesgue measure in R. Then, almost surely, the following conver-
gence holds:

M (X ) = lim

τ →0+

S (X/τ ).

(4)

Finally, we note that Theorem 1 cannot be realized in practice, as it involves a limit on the Sinkhorn
iterations l. Instead, we’ll always consider the incomplete version of the Sinkhorn operator (Adams
& Zemel, 2011), where we truncate l in (1) to L. Figure 3b in appendix A.3 illustrates the depen-
dence of the approximation in τ and L.

3 S INKHORN N ETWORK S

Now we show how to apply the approximation in Theorem 1 in the context of artiﬁcial neural
networks. We construct a layer that encodes the representation of a permutation, and show how to
train networks containing such layers as intermediate representations.
We deﬁne the components of this network through a minimal example: consider the supervised task
of learning a mapping from scrambled objects ˜X to actual, non-scrambled X . Data, then, are M
pairs (Xi , ˜Xi ) where ˜Xi can be constructed by randomly permuting pieces of Xi . We state this
problem as a permutation-valued regression Xi = P −1
˜Xi + εi , where εi is a noise term, and
is the permutation matrix mapping Xi to ˜Xi , which depends on ˜Xi and parameters θ . We are
concerned with minimization of the reconstruction error 3 :
||Xi − P −1

f (θ , X, ˜X ) =

M(cid:88)

˜Xi ||2 .

Pθ, ˜Xi

θ, ˜Xi

(5)

i=1

θ, ˜Xi

One way to express a complex parameterization of this kind is through a neural network: this net-
work receives ˜Xi as input, which is then passed through some intermediate, feed-forward compu-
tations of the type gh (Whxh + bh ), where gh are nonlinear activation functions, xh is the output
of a previous layer, and θ = {(Wh , bh )}h are the network parameters. To make the ﬁnal network
output be a permutation, we appeal to constructions developed in Section 2: by assuming that the
ﬁnal network output Pθ, ˜X can be parameterized as the solution of the assignments problem; i.e.,
Pθ, ˜X = M (g( ˜X , θ)), where g(·, θ) represents the outcome of all operations involving gh .
Unfortunately, the above construction involves a non-differentiable f (in θ). We use Theorem 1 as
a justiﬁcation for replacing M (g( ˜X , θ)) by the differentiable S (g( ˜X , θ)/τ ) in the computational
graph. The value of τ must be chosen with caution: if τ is too small, gradients vanishes almost
everywhere, as S (g( ˜X , θ)/τ ) approaches the non-differentiable M (g( ˜X , θ)). Conversely, if τ is too
large, S (X/τ ) may be far from the vertices of the Birkhoff polytope, and reconstructions P −1
may be nonsensical (see Figure 2a).
Importantly, we will always add noise to the output layer
g( ˜X , θ) as a regularization device: by doing so we ensure uniqueness of M (g( ˜X , θ)), which is
required for convergence in Theorem 1.

θ, ˜X

˜X

3 .1 P ERMU TAT ION EQU IVAR IANC E

Among all possible architectures that respect the aforementioned parameterization, we will only
consider networks that are permutation equivariant, the natural kind of symmetry arising in this
= P (cid:48) (cid:16)
context. Speciﬁcally, we require networks to satisfy:
where P (cid:48) is an arbitrary permutation. The underlying intuition is simple: reconstructions of objects
should not depend on how pieces were scrambled, but only on the pieces themselves. We achieve
permutation equivariance by using the same network to process each piece of ˜X , throwing an N

Pθ,P (cid:48) ˜X

P (cid:48) ˜X

Pθ, ˜X

(cid:16)

(cid:17)

(cid:17)

˜X

3This error arises from gaussian εi . Other choices may be possible, but here we stick to the most straight-
forward formulation

3

Published as a conference paper at ICLR 2018

dimensional output. Then, these N outputs (each with N components) are used to create the rows
of the matrix g( ˜X , θ), to which we ﬁnally apply the (differentiable) Sinkhorn operator (i.e. g stacks
the composition of the gh acting locally on each piece). One can interpret each row as representing
a vector of local likelihoods of assignment, but they might be inconsistent. The Sinkhorn operator,
then, mixes those separate representations, and ensures that consistent (approximate) assignment
are produced. With permutation equivariance, the only consideration left to the practitioner is the
choice of the particular architecture, which will depend on the particular kind of data. In Section 5
we illustrate the uses of Sinkhorn networks with three examples, each of them using a different
architecture. Also, in ﬁgure 1 we illustrate a network architecture used in one of our examples.

3 .2 SUMMARY

Sinkhorn network is a supervised method for learning to reconstruct a scrambled object ˜X (input)
given several training examples (Xi , ˜Xi ). By applying some non-linear transformations, a Sinkhorn
network richly parameterizes the mapping between ˜X and the permutation P that once applied to
˜X , will allow to reconstruct the original object as Xrec = P (cid:62) ˜X (the output). We note that Sinkhorn
networks may be similarly used not only to learn permutations, but also to learn matchings between
objects of two sets of the same size.

Figure 1: Schematic of Sinkhorn Network for Jigsaw puzzles. Each piece of the scrambled digit ˜X is
processed with the same (convolutional) network g1 (arrows with solid circles). The outputs lying on
a latent space (rectangles surrounding ˜X ) are then connected through g2 (arrows with empty circles)
to conform the rows of the matrix g( ˜X , θ); g( ˜X , θ)i = g1 ◦ g2 ( ˜Xi ). Rows may be interpreted as
unnormalized assignment probabilities, indicating individual unnormalized likelihoods of pieces of
˜X to be at every position in the actual image. Applying S (·) leads to a ‘soft-permutation’ Pθ, ˜X that
resolves inconsistencies in g( ˜X , θ). Pθ, ˜X is then used to recover the actual X at training, although
at test time one may use the actual M (g( ˜X , θ)).

4 PROBAB I L I S T IC A S PEC T S : THE GUMB EL -S INKHORN AND
GUMB EL -MATCH ING D I S TR IBUT ION S

Recently, in Jang et al. (2016) and Maddison et al. (2016), the Gumbel-Softmax or Concrete distri-
butions were deﬁned for computational graphs with stochastic nodes; i.e, latent probabilistic repre-
sentations. Their choice is guided by the following i) they seek re-parameterizable distributions to
enable the re-parameterization trick (Kingma & Welling, 2013), and note that via the Gumbel trick
(see below) any categorical distribution is re-parameterizable, ii) since the re-parameterization in i)

4

Published as a conference paper at ICLR 2018

is not differentiable, they consider instead sampling under the softmax approximation. This gives
rise to the Gumbel-Softmax distribution.
Here we parallel these choices to enable learning of a probabilistic latent representation of permuta-
tions. To this aim, we start by considering a generic distribution on the discrete set Y , with potential
function X : Y → R:

p(y |X ) ∝ exp (X (y)) 1y∈Y .

(6)

i=1 Yi

γ (y) = (cid:80)N

Regarding i), the Gumbel trick arises in the context of Perturb and MAP methods (Papandreou
& Yuille, 2011) for sampling in discrete graphical models. This has recently received renewed
interest (Balog et al., 2017), as it recasts the a difﬁcult sampling problem as an easier optimization
of each potential X (y), with Gumbel i.i.d. noise γ (y); i.e., arg maxy∈Y {X (y) + γ (y)} ∼ p(·|X ).
problem. In detail, sampling from (6), can be achieved by the maximization of random perturbations
Therefore, one can re-parameterize any categorical distribution (corresponding to (6) with X (y) =
(cid:104)X, y(cid:105)) by the choice of a category, after injecting noise.
an interesting result: in cases where Y factorizes, Y = (cid:81)N
However, the above scheme is unfeasible in our context, as |Y | = N !. Nonetheless, we appeal to
4 , the use of rank-one perturbations
i=1 γi (yi ) is proposed as a more tractable alternative. Although ultimately heuristic, they
lead to bounds in the partition function (Hazan & Jaakkola, 2012; Balog et al., 2017), and can also
be understood as providing approximate or unbiased samples from the true density (Hazan et al.,
2013; Tomczak, 2016).
Guided by this, we say the random permutation P follows the Gumbel-Matching distribution with
parameter X , denoted P ∼ G .M.(X ), if it has the distribution arising by the rank-one perturbation
of (6) on permutations, with the linear potential X (P ) = (cid:104)X, P (cid:105)F (replacing y with P ). One can
verify, in a similar line as in Li et al. (2013), that M (X + ε) ∼ G .M.(X ), if ε is a matrix of standard
i.i.d. Gumbel noise.
Unfortunately, as ii) with the categorical case, Gumbel-Matching distribution samples are not differ-
entiable in X , but by appealing to Theorem 1, we deﬁne its relaxation for doubly stochastic matrices
as follows: we say P follows the Gumbel-Sinkhorn distribution with parameter X and temperature
τ , denoted P ∼ G .S .(X, τ ), if it has the distribution of S ((X + ε)/τ ). Samples of G .S .(X, τ )
converge almost surely to samples of the Gumbel-Matching distribution (see Fig 3c in appendix
A.3).
Unlike for the categorical case, neither the Gumbel-Matching nor Gumbel-Sinkhorn distributions
have tractable densities. However, this does not preclude inference: likelihood-free methods have
recently been developed to enable learning in such implicitly deﬁned distributions (Ranganath et al.,
2016; Tran et al., 2017). These methods avoid evaluating the likelihood based on the observation
that in many cases inference can be cast as the estimation of a likelihood ratio, which can be obtained
from samples (Husz ´ar, 2017). Regardless of these useful advances, in the following we develop a
solution based on using the likelihoods of random variables whose densities are available.

4 .1 A P PROX IMATE PO ST ER IOR IN FERENCE

Consider a latent variable model probabilistic model with observed data Y , and latent Z = {P , W }
where P is a permutation and W are other variables. Here we illustrate how to approximate the
posterior probability p({P , W }|Y ) using variational inference Blei et al. (2017). Speciﬁcally, we
aim to maximize the ELBO, the r.h.s. of (7):

log p(y) ≥ Eq(Z |Y ) (log p(Y |Z )) − K L(q(Z |Y ) (cid:107) p(Z )).

(7)

We assume that both the prior and variational posteriors decompose as products (mean-ﬁeld). That
is, q({P , W }|Y ) = q(P )q(W ), p(P , W ) = p(P )p(W ). With this assumption, we may focus only
on the discrete part of the problem, i.e. without loss of generality we can assume Z = P .
with some parameter X ; G .M.(X ). To enable differentiability, we replace them by G .S .(X, τ )
We parameterize our variational prior and posteriors on P using the Gumbel-Matching distributions
distributions, leading to a surrogate ELBO that uses relaxed (continuous) variables. In more detail,

4 It sufﬁces that Y is a subset of the product space, which here is true as Y = Pn ⊆ {1, . . . , N }N .

5

Published as a conference paper at ICLR 2018

Test distribution

N = 5 N = 10 N = 15 N = 80 N = 100 N = 120

U (0, 1)

U (0, 1) (Vinyals et al., 2015)

U (0, 10)
U (0, 1000)
U (1, 2)
U (10, 11)
U (100, 101)
U (1000, 1001)

.0

.06
.0
.0
.0
.0
.0
.0

.0

0.43
.0
.0
.0
.0
.0
.0

.0

0.9
.0
.0
.0
.0
.01
.07

.0

-
.0
.01
.01
.08
.02
1.

.0

-
.02
.02
.04
.08
.99
1.

.01

-
.03
.04
.08
.6
1.
1.

Table 1: Results on the number sorting task measured using Prop. any wrong. In the top two rows we
compare to Vinyals et al. (2015), showing that our approach can sort far more inputs at signiﬁcantly
higher accuracy. In the bottom rows we evaluate generalization to different intervals on the real line.

for our uniform prior over permutations we use the isotropic G .S .(X = 0, τprior ) distribution, while
for the variational posterior we consider the more generic G .S .(X, τ ).

Unfortunately, the term K L(q(P |Y ) (cid:107) p(P )) = K L(G .S .(X, τ ) (cid:107) G .S .(X = 0, τprior )) in equa-

tion (7) is intractable as there is not closed form expression for the density of G .S . random vari-
ables. As a solution, we use that our prior and posterior are re-parameterizable in terms of matrices
ε of Gumbel i.i.d variables: we have S ((X + ε)/τ ) ∼ G .S .(X, τ ) and S (ε/τprior ) ∼ G .S .(X =
0, τprior ), for the posterior and prior, respectively. To obtain a tractable expression, we propose to
use as ‘code’ or stochastic node Z , the variable (X + ε)/τ instead. Then, the KL term substan-
tially simpliﬁes to K L((X + ε)/τ (cid:107) ε/τprior ). This term can be computed explicitly, as shown in
appendix B.3.
This ‘trick’, however, comes at a cost:
the divergence K L(Z1 (cid:107) Z2 ) would certainly remain
unchanged by applying the same invertible transformation g to both variables Z1 and Z2 , but
in the general case, for non-invertible transformations, such as S (·), one has K L(Z1 (cid:107) Z2 ) ≥
K L(g(Z1 ) (cid:107) g(Z2 )). This implies that working in the ‘Gumbel space’ might entail the optimization
of a less tight lower bound. Nonetheless, through categorical experiments on MNIST (see appendix
C.3) we observe this loss of tightness is minimal, suggesting the suitability of our approach on per-
and posterior were the same function (S (·)) of a simpler distribution. This may not be the case in
mutations. Finally, we note that key to to our treatment of the problem is the fact that both the prior
more general models.
To conclude this section, we refer the reader to table 8 in appendix D.2 for a summary of all the
constructions on permutations developed in this work.

5 EX PER IM EN T S

In this section we perform several experiments comparing to existing methods. In the ﬁrst three ex-
periments we explore different Sinkhorn network architectures of increasing complexity, and there-
fore, they mostly implements section 3. The fourth experiment relates to the probabilistic construc-
tions described in section 4, and addresses a problem involving marginal inferences over a latent,
unobserved permutation. All experimental details not stated here are in appendix B.

5 .1 SORT ING NUMBER S

To illustrate the capabilities of Sinkhorn Networks in a simple scenario, we consider the task of
sorting numbers using artiﬁcial neural networks as in Vinyals et al. (2015). Speciﬁcally, we sample
uniform random numbers ˜X in the [0, 1] interval and we train our network with pairs ( ˜X , X ) where
X are the same ˜X but in sorted order. The network has a ﬁrst fully connected layer that links a
number with an intermediate representation (with 32 units), and a second (also fully connected)
layer that turns that representation into a row of the matrix g( ˜X , θ).
Table 1 shows our network learns to sort up to N = 120 numbers. As an evaluation measure, we re-
port the proportion of sequences where there was at least one error (Prop. any wrong). Surprisingly,

6

Published as a conference paper at ICLR 2018

MNIST
4x4
.43

-
.45
.97
.04
.26

2x2
1.

-
.0
.0
.0
.0

3x3
.83

-
.09
.28
.0
.0

5x5
.39

-
.45
1.
.02
.18

6x6
.27

-
.59
1.
.03
.19

2x2
1.0

-
.0
.0
.0
.0

Celeba
3x3
4x4
.96
.88

-
.03
.09
.01
.11

-
.1
.36
.04
.18

Imagenet
2x2
3x3
.85

.73

-
.12
.19
.05
.22

.72
.26
.53
.12
.31

5x5
.78

-
.21
.73
.08
.24

Kendall tau
Kendall tau
(Cruz et al., 2017)
Prop. wrong
Prop. any wrong

l1
l2

Table 2: Jigsaw puzzle results. We compare to the available result on the Kendall Tau metric
permutations of n items have an expected proportion of errors of (n − 1)/n. Note that our model
from Cruz et al. (2017) and provide additional results from our experiments. Randomly guessed
has at least 20x fewer parameters..

the network learns to sort numbers even when test examples are not sampled from U (0, 1), but on
a considerably different interval. This indicates the network is not overﬁtting. These results can be
compared with those from Vinyals et al. (2015), where a much more complex (recurrent) network
was used, but performance guarantees were obtained only with at most N = 15 numbers. In that
case, the reported error rate is 0.9, whereas ours starts to degrade only after N ≈ 100 for most test
intervals.

5 .2

J IG SAW PU ZZ LE S

A more complex scenario for learning permutations arises in the reconstruction of an image X from
a collection of scrambled “jigsaw” pieces ˜X (Noroozi & Favaro, 2016; Cruz et al., 2017). In this
example, our network differs from the one in 5.1 in the ﬁrst layer is a simple CNN (convolution
+ max pooling), which maps the puzzle pieces to an intermediate representation (see ﬁgure 1 for
details).
For evaluation on test data, we report several measures: ﬁrst, in addition to Prop. any wrong we
also consider Prop. wrong, the overall proportion of scrambled pieces that were wrongly assigned
to their actual position. Also, we use l1 and l2 (train) losses and the Kendall tau, a “correlation
coefﬁcient” for ranked data. In Table 2, we benchmark results for the MNIST, Celeba and Imagenet
datasets, with puzzles between 2x2 and 6x6 pieces. In MNIST we achieve very low l1 and l2 on up
to 6x6 puzzles but a high proportion of errors. This is a consequence of our loss being agnostic to
particular permutations, but only caring about reconstruction errors: as the number of black pieces
increases with the number of puzzle pieces, many become unidentiﬁable under this loss.
In Celeba, we are able to solve puzzles of up to 5x5 pieces with only 21% of pieces of faces being
incorrectly ordered (see Figure 2a for examples of reconstructions). For this dataset, we provide
additional baselines in Table 4 of appendix C.1: there, we show that performance substantially de-
creases if the temperature is too small or large, but only slightly decreases if only one Sinkhorn
iterations is made. We observe that temperature does play a relevant role, consistent with the ﬁnd-
ings of Maddison et al. (2016); Jang et al. (2016). This might not be obvious a-priori, as one could
reason that temperature over-parameterizes the network. However, results conﬁrm this is not the
case. We hypothesize that different temperatures result in parameter convergence in different phases
or regions. Also, the minor difference for a single iteration suggest that only a few might be neces-
sary, implying potential savings in the memory needed to unroll computations in the graph, during
training.
Learning in the Imagenet dataset is much more challenging, as there isn’t a sequential structure that
generalizes among images, unlike Celeba and MNIST. In this dataset, our network ties with the .72
Kendall tau score reported in (Cruz et al., 2017). Their network, named DeepPermNet, is based on
the stacking of up to the sixth fully connected layer fc6 of AlexNet (Krizhevsky et al., 2012), which
ﬁnally (fully) connects to a Sinkhorn layer through intermediate fc7 and fc8. We note, however, our
network is much simpler, with only two layers and far fewer parameters. Speciﬁcally, the network

7

Published as a conference paper at ICLR 2018

Figure 2: (a) Sinkhorn networks can be trained to solve Jigsaw Puzzles. Given a trained model, ‘soft’
reconstructions are shown at different τ using S (X/τ ). We also show hard reconstructions, made by
computing M (X ) with the Hungarian algorithm (Munkres, 1957). (b) Sinkhorn networks can also
be used to learn to transform any MNIST digit into another. We show hard and soft reconstructions,
with τ = 1.

that produced our best results had around 1,050,000 parameters (see appendix B for a derivation),
while in DeepPermNet, the layer connecting fc6 with fc7 has 512 × 4096 × 9 ≈ 19, 000, 000
parameters, let alone the AlexNet parameters (also to be learned). Indeed, we believe there is no
reason to consider a complex stacking of convolutions: as the number of pieces increases, each
piece is smaller and the convolutional layer eventually becomes fully connected. In the following
experiment we explore this phenomenon in more detail.

5 .3 A S S EMBLY O F ARB I TRARY MN IST D IG I T S FROM P I ECE S

We also consider an original application, motivated by the observation that the Jigsaw Puzzle task
becomes ill-posed if a puzzle contains too many pieces. Indeed, consider the binarized MNIST
dataset: there, reconstructions are not unique if pieces are sufﬁciently atomic, and in the limit case
of pieces of size 1x1 squared pixels, for a given scrambled MNIST digit there are as many valid
reconstructions as there are MNIST digits with the same number of white pixels. In other words,
reconstructions stop being probabilistic and become a multimodal distribution over permutations.
We exploit this intuition to ask whether a neural network can be trained to achieve arbitrary digit
reconstructions, given their loose atomic pieces. To address this question, we slightly changed the
network in 5.2, this time stacking several second layers linking an intermediate representation to
the output. We trained the network to reconstruct a particular digit with each layer, by using digit
identity to indicate which layer should activate with a particular training example.
Our results demonstrate a positive answer: Figure 2b shows reconstructions of arbitrary digits given
10x10 scrambled pieces. In general, they can be unambiguously identiﬁed by the naked eye. More-
over, this judgement is supported by the assessment of a neural network. Speciﬁcally, we trained a
two-layer CNN 5 on MNIST (achieving a 99.2% accuracy on test set) and evaluated its performance
on the test set generated by arbitrary transformations of each digit of the original test set into any
other digit. We found the CNN made an appropriate judgement in 85.1% of the time. More speciﬁc
results, regarding speciﬁc transformations are presented in Table 5 of appendix C.2.
Finally, we note that meaningful assemblies are possible regardless of the original digit: in Fig-
ure 4 of appendix C.2 we show arbitrary reconstructions, by this same network, of “digits” from
a ‘strongly mixed’ MNIST dataset. In detail, these “digits” were crafted by sampling, without re-
placement, from a bag containing all the small pieces from all original digits. These reconstructions
suggest the possibility of an alternative to generative modeling, based on the (random) assembly of
small pieces of noise, instead of the processing of noise through a neural network. However, this
would require training the network without supervision, which is beyond the scope of this work.

5Speciﬁcally, we used the one described in the Deep MNIST for experts tutorial.

8

Published as a conference paper at ICLR 2018

Prop. known neurons
Difﬁculty
MCMC
(Linderman et al., 2017)
Gumbel-Sinkhorn
Gumbel-Sinkhorn, no regularization

.97
.97

.96

40.%
30.%
20.%
10.%
Easy Hard Easy Hard Easy Hard Easy Hard
.85
.82
.51
.44
.29
.27
.16
.12
.95
.90
.39
.21

.85

.77

.96

.93

.92

.89

.84
.78

.76
.71

.59
.59

.52

.44

.4

.26

.23

Table 3: Results for the C. elegans neural inference problem.

5 .4 PO ST ER IOR IN FERENCE OV ER PERMU TAT ION S W I TH TH E GUMB EL -S INKHORN
E ST IMATOR

We illustrate how the G .S . distribution can be used as a continuous relaxation for stochastic nodes
in a computational graph. To this end, we revisit the “C. elegans neural identiﬁcation problem”,
originally introduced in Linderman et al. (2017). We refer the reader to (Linderman et al., 2017)
for an in-depth introduction, but brieﬂy, C. elegans is a nematode (worm) whose biological neural
conﬁguration – the connectome – is stereotypical; i.e. specimens always posses the same number
of somatic neurons (282) (Varshney et al., 2011), and the ways those neurons connect and interact
changes little from worm to worm. Therefore, its brain can be thought of as a canonical object, and
its neurons can unequivocally be identiﬁed with names.
The task, then, consists of matching traces from the observed neural dynamics Y to identities (neu-
ron names) in the canonical brain. This problem is stated in terms of a Bayesian hierarchical model,
in order to proﬁt from prior information that may constrain the possibilities. Speciﬁcally, one states
a linear dynamical system Yt = P W P TYt−1 + νt , where νt is a noise term and W and P are
latent variables with respective prior distributions. W encodes the dynamics, with a prior p(W ) to
represent the sparseness of the connectome, etc., and P is a permutation matrix representing the
matching between indexes of observed neurons and their canonical counterparts, where we place a
ﬂat prior p(P ) over permutations. Notably, within the framework it is possible to model the simul-
taneous problem with many worms sharing the same dynamical system, but here we avoid explicit
references to individuals for notational ease.
Given this model, we seek the posterior distribution p({P , W }|Y ), a problem that we address with
variational inference (Blei et al., 2017) using the constructions developed in 4.1. In Table 3 (and
also in Table 7 of appendix C.4) we show results for this task, using accuracy in matching as the
performance measure. These are broken down by relevant experimental covariates (Linderman et al.,
2017): different proportion of neurons known beforehand, and by task difﬁculty. As baselines, we
include i) a simple MCMC sampler that proposes local swipes on permutations ii) the rounding
method presented in Linderman et al. (2017), iii) our method, where we also consider the absence
of regularization. Results show our method outperforms the alternatives in most cases. MCMC
fails because mixing is poor, but differences are much subtler with the other baselines. With them,
we see that clear differences with the no-regularization case conﬁrm the stochastic nature of this
problem, i.e., that it is truly necessary to represent a latent probabilistic permutation. We believe
our method outperforms the one in Linderman et al. (2017) because theirs, although it provides a
explicit density, is a less tight relaxation, in the sense that points can be anywhere in the space, and
not only on the Birkhoff polytope. Therefore, their prior also needs to be deﬁned on the entire space
and may not property act as an efﬁcient regularizer.

6 R ELAT ED WORK

Learning with matchings has been extensively been studied in the machine learning community;
but current applications mostly relate to structured prediction (Petterson et al., 2009; Tang et al.,
2016). However, our probabilistic treatment focuses on marginal inference in a model with a latent
matching. This is a more challenging scenario, as standard learning techniques, i.e. the score func-
tion estimator or REINFORCE (Williams, 1992), are not applicable due to the partition function for
non-trivial distributions over matchings.

9

Published as a conference paper at ICLR 2018

In the case of latent categories, a recent technique that combines a relaxation and the re-
parameterization trick (Kingma & Welling, 2013) was proposed as a competitive alternative to RE-
INFORCE for the marginal inference scenario. Speciﬁcally, Maddison et al. (2016); Jang et al.
(2016) use the Gumbel-trick to re-parameterize a discrete density, and then replace it with a re-
laxed surrogate, the Gumbel Softmax distribution, to enable gradient-descent. Our work, like the
simultaneous work of Linderman et al. (2017), aims to extends the scope of this technique to latent
permutations. We deem our Gumbel Sinkhorn distributions as the most natural tractable extension
of the Gumbel Softmax to permutations, as we clearly parallel each of the steps leading to its con-
struction. A parallel is also presented in Linderman et al. (2017); and notably, unlike ours, their
framework produces tractable densities. However, it is less clear how their constructions extend each
of the features of the Gumbel Softmax: for example, their rounding-based relaxation also utilizes
the Sinkhorn operator, but the limit they consider does not make use of the non-trivial statement of
Theorem 1, which naturally extends the categorical case (see appendix A.2 for details). In practice,
we see our results favor the Gumbel Sinkhorn distribution, since it is a tighter relaxation.
Connections between permutations and the Sinkhorn operator have been known for at least twenty
years. Indeed, the limit in Theorem 1 was ﬁrst presented in Kosowsky & Yuille (1994), but their
interpretation and motivation were more linked to statistical physics and economics. However, our
approach is different and links to recent developments in optimal transport (OT) (Villani, 2003):
Theorem 1 draws on the entropy-regularization for OT technique developed inCuturi (2013), where
the entropy-regularized transportation problem is referred to as a ‘Sinkhorn distance’. The exten-
sion is sensible as in the case of transportation between two discrete measures (here) the Birkhoff
polytope appears naturally as the optimization set (Villani, 2003). Entropy regularization as means
to achieve a differentiable version of a loss was ﬁrst proposed in Genevay et al. (2017) in the con-
text of generative modeling. Although this ﬁeld may appear separate, recent work (Salimans et al.,
2018) makes explicit the connection to permutations: to compute a (Wasserstein) distance between
a batch of dataset samples and one of generative samples of the same size, one needs to solve the
matching problem so that the distance between matched samples is minimized. Finally, we note our
work shares with Salimans et al. (2018); Genevay et al. (2017) in that the OT cost function (here,
the matrix X ) is learned using an artiﬁcial neural network.
We understand our work as extending Adams & Zemel (2011), which developed neural networks to
learn a permutation-like structure; a ranking. However, there, as in Helmbold & Warmuth (2009),
of a matrix of the marginals, i.e., S (P ) ≈ E (P ). In consequence, there was no need to introduce
the objective function was linear and the Sinkhorn operator was instead used as an approximation
a temperature parameter and consider a limit argument, which is critical to our case. Interestingly,
equation (10) can be understood in terms of approximate marginal inference, justifying the approx-
imation S (P ) ≈ E (P ). We comment on this in appendix D.1. Note that Sinkhorn iteration can
be interpreted as mean-ﬁeld inference in an associated Gibbs distribution over matchings. With
this in mind, backpropagation through Sinkhorn is an end-to-end learning in an unrolled inference
algorithm Stoyanov et al. (2011); Domke (2013). In future work, it may be fruitful to unroll alterna-
tive algorithms for marginal inference over matchings, such as belief propagation (Huang & Jebara,
2009).
Sinkhorn networks were also very recently introduced in Cruz et al. (2017), although their work
substantially differs from ours. While their interest lies in the representational aspects of CNN’s,
we are more concerned with the more fundamental properties. In their work, they don’t consider
a temperature parameter τ , but their network still successfully learns, as τ = 1 happens to fall
within the range of reasonable values. On the Jigsaw puzzle task, we showed that we achieve
equivalent performance with a much simpler network having several times fewer parameters and
layers. Nonetheless, we recognize the need for more complex architectures for the tasks considered
in Cruz et al. (2017), and we hope our more general theory; particularly, Theorem 1 and the notion
of equivariance, may aid further developments in that direction.

7 D I SCU S S ION

We have demonstrated Sinkhorn networks are able to learn to ﬁnd the right permutation in the most
elementary cases; where all training samples obey the same sequential structure; e.g., in sorted
number and in pieces of faces, as we expect parts of faces occupy similar positions from sample to

10

Published as a conference paper at ICLR 2018

sample. This is already non-trivial, as indicates one can train a neural network to solve the linear
assignment problem.
However, the fact that Imagenet represented a much more challenging scenario indicates there are
clear limits to our formulation. As the most obvious extension we propose to introduce a sequential
stage, in which current solutions are kept on a memory buffer, and improved. One way to achieve this
would be by exploring more complex parameterizations for permutations; i.e. replacing M (X ) by
a quadratic operator that may parameterize a notion of local distance between pieces. Alternatively,
one may resort to reinforcement learning techniques, as suggested in Bello et al. (2016). Either
sequential improvement would help solve the “Order Matters” problem (Vinyals et al., 2015), and
we deem our elementary work as a signiﬁcant step in that direction.
We have made available Tensorﬂow code for Gumbel-Sinkhorn networks featuring an implementa-
tion of the number sorting experiment at http://github.com/google/gumbel sinkhorn .

R E F ER ENC E S

Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorﬂow: Large-scale machine
learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.

Ryan Prescott Adams and Richard S Zemel. Ranking via sinkhorn propagation. arXiv preprint
arXiv:1106.1925, 2011.

Matej Balog, Nilesh Tripuraneni, Zoubin Ghahramani, and Adrian Weller. Lost relatives of the
gumbel trick. arXiv preprint arXiv:1706.04161, 2017.

Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio. Neural combinatorial
optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016.

Garrett Birkhoff. Tres observaciones sobre el algebra lineal. Univ. Nac. Tucum ´an. Revista A, 5:
147–151, 1946.

David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisti-
cians. Journal of the American Statistical Association, (just-accepted), 2017.

Matko Boˇsnjak, Tim Rockt ¨aschel, Jason Naradowsky, and Sebastian Riedel. Programming with a
differentiable forth interpreter. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th
International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning
Research, pp. 547–556, International Convention Centre, Sydney, Australia, 06–11 Aug 2017.

PMLR. URL http://proceedings.mlr.press/v70/bosnjak17a.html.

Tib ´erio S Caetano, Julian J McAuley, Li Cheng, Quoc V Le, and Alex J Smola. Learning graph
matching.
IEEE transactions on pattern analysis and machine intelligence, 31(6):1048–1058,
2009.

Roberto Cominetti and Jaime San Mart´ın. Asymptotic analysis of the exponential penalty trajectory
in linear programming. Mathematical Programming, 67(1-3):169–187, 1994.

Rodrigo Santa Cruz, Basura Fernando, Anoop Cherian, and Stephen Gould. Deeppermnet: Visual
permutation learning. arXiv preprint arXiv:1704.02729, 2017.

Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in
neural information processing systems, pp. 2292–2300, 2013.

Justin Domke. Learning graphical model parameters with approximate marginal inference. IEEE
transactions on pattern analysis and machine intelligence, 35(10):2454–2467, 2013.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, and Noah A Smith. Transition-based
dependency parsing with stack long short-term memory. arXiv preprint arXiv:1505.08075, 2015.

Alexander L Gaunt, Marc Brockschmidt, Rishabh Singh, Nate Kushman, Pushmeet Kohli, Jonathan
Taylor, and Daniel Tarlow. Terpret: A probabilistic programming language for program induction.
arXiv preprint arXiv:1608.04428, 2016.

11

Published as a conference paper at ICLR 2018

Aude Genevay, Gabriel Peyr ´e, and Marco Cuturi. Learning generative models with sinkhorn diver-
gences. arXiv preprint arXiv:1706.00292, 2017.

Amir Globerson and Tommi Jaakkola. Approximate inference using conditional entropy decompo-
sitions. In International Conference on Artiﬁcial Intelligence and Statistics, pp. 130–138, 2007.

Alex Graves, Greg Wayne, and Ivo Danihelka. Neural
arXiv:1410.5401, 2014.

turing machines.

arXiv preprint

Tamir Hazan and Tommi Jaakkola. On the partition function and random maximum a-posteriori
perturbations. arXiv preprint arXiv:1206.6410, 2012.

Tamir Hazan, Subhransu Maji, and Tommi Jaakkola. On sampling from the gibbs distribution with
random maximum a-posteriori perturbations. In Advances in Neural Information Processing Sys-
tems, pp. 1268–1276, 2013.

David P Helmbold and Manfred K Warmuth. Learning permutations with exponential weights.
Journal of Machine Learning Research, 10(Jul):1705–1736, 2009.

Bert Huang and Tony Jebara. Approximating the permanent with belief propagation. arXiv preprint
arXiv:0908.1769, 2009.

Ferenc Husz ´ar. Variational inference using implicit distributions. arXiv preprint arXiv:1702.08235,
2017.

Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144, 2016.

Armand Joulin and Tomas Mikolov.
Inferring algorithmic patterns with stack-augmented
recurrent nets.
In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama,
and
R. Garnett
(eds.), Advances in Neural
Information Processing Systems 28, pp. 190–
198. Curran Associates,
Inc.,
2015.

URL http://papers.nips.cc/paper/
5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets.
pdf.

Diederik P Kingma and Max Welling. Auto-encoding variational bayes.
arXiv:1312.6114, 2013.

arXiv preprint

Philip A Knight. The sinkhorn–knopp algorithm: convergence and applications. SIAM Journal on
Matrix Analysis and Applications, 30(1):261–275, 2008.

JJ Kosowsky and Alan L Yuille. The invisible hand algorithm: Solving the assignment problem with
statistical physics. Neural networks, 7(3):477–490, 1994.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097–1105,
2012.

Harold W Kuhn. The hungarian method for the assignment problem. Naval Research Logistics
(NRL), 2(1-2):83–97, 1955.

Ke Li, Kevin Swersly, Ryan, and Richard S Zemel. Efﬁcient feature learning using perturb-and-map.
NIPS Workshop on Perturbations, Optimization, and Statistics, 2013.

Scott W Linderman, Gonzalo E Mena, Hal Cooper, Liam Paninski, and John P Cunningham.
Reparameterizing the birkhoff polytope for variational permutation inference. arXiv preprint
arXiv:1710.09508, 2017.

Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.

James Munkres. Algorithms for the assignment and transportation problems. Journal of the society
for industrial and applied mathematics, 5(1):32–38, 1957.

12

Published as a conference paper at ICLR 2018

Arvind Neelakantan, Quoc V Le, and Ilya Sutskever. Neural programmer: Inducing latent programs
with gradient descent. arXiv preprint arXiv:1511.04834, 2015.

Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw
puzzles. In European Conference on Computer Vision, pp. 69–84. Springer, 2016.

George Papandreou and Alan L Yuille. Perturb-and-map random ﬁelds: Using discrete optimization
to learn and sample from energy models. In Computer Vision (ICCV), 2011 IEEE International
Conference on, pp. 193–200. IEEE, 2011.

James Petterson, Jin Yu, Julian J McAuley, and Tib ´erio S Caetano. Exponential family graph match-
ing and ranking. In Advances in Neural Information Processing Systems, pp. 1455–1463, 2009.

Rajesh Ranganath, Dustin Tran, Jaan Altosaar, and David Blei. Operator variational inference. In
Advances in Neural Information Processing Systems, pp. 496–504, 2016.

C Radhakrishna Rao. Convexity properties of entropy functions and analysis of diversity. Lecture
Notes-Monograph Series, pp. 68–77, 1984.

Ralph Tyrell Rockafellar. Convex analysis. Princeton university press, 1970.

Tim Rockt ¨aschel and Sebastian Riedel.
arXiv:1705.11040, 2017.

End-to-end differentiable proving.

arXiv preprint

Tim Salimans, Han Zhang, Alec Radford, and Dimitris Metaxas. Improving GANs using optimal
transport.
In International Conference on Learning Representations, 2018. URL https://

openreview.net/forum?id=rkQkBnJAb.

Richard Sinkhorn. A relationship between arbitrary positive matrices and doubly stochastic matri-
ces. The annals of mathematical statistics, 35(2):876–879, 1964.

Richard Sinkhorn and Paul Knopp. Concerning nonnegative matrices and doubly stochastic matri-
ces. Paciﬁc Journal of Mathematics, 21(2):343–348, 1967.

Veselin Stoyanov, Alexander Ropson, and Jason Eisner. Empirical risk minimization of graphical
model parameters given approximate inference, decoding, and model structure. In Proceedings
of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics, pp. 725–733,
2011.

Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. In Advances
in neural information processing systems, pp. 2440–2448, 2015.

Kui Tang, Nicholas Ruozzi, David Belanger, and Tony Jebara. Bethe learning of conditional random
ﬁelds via map decoding. AISTATS, 2016.

Jakub M Tomczak. On some properties of the low-dimensional gumbel perturbations in the perturb-
and-map model. Statistics & Probability Letters, 115:8–15, 2016.

Dustin Tran, Rajesh Ranganath, and David M Blei. Deep and hierarchical implicit models. arXiv
preprint arXiv:1702.08896, 2017.

Lav R Varshney, Beth L Chen, Eric Paniagua, David H Hall, and Dmitri B Chklovskii. Structural
properties of the caenorhabditis elegans neuronal network. PLoS computational biology, 7(2):
e1001066, 2011.

C ´edric Villani. Topics in optimal transportation. Number 58. American Mathematical Soc., 2003.

Luke Vilnis, David Belanger, Daniel Sheldon, and Andrew McCallum. Bethe projections for non-
local inference. arXiv preprint arXiv:1503.01397, 2015.

Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for sets.
arXiv preprint arXiv:1511.06391, 2015.

Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational
inference. Foundations and Trends R(cid:13) in Machine Learning, 1(1–2):1–305, 2008.

13

Published as a conference paper at ICLR 2018

Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229–256, 1992.

Jonathan S Yedidia, William T Freeman, and Yair Weiss. Bethe free energy, kikuchi approximations,
and belief propagation algorithms. Advances in neural information processing systems, 13, 2001.

14

Published as a conference paper at ICLR 2018

A PROO F O F TH EOR EM 1

In this section we give a rigorous proof of Theorem 1. Also, in A.2 we brieﬂy comment on how
Theorem 1 extend a perhaps more intuitive results, in the probability simplex.
Before stating Theorem 1 we need some preliminary deﬁnitions. We start by recalling a well-known
result in matrix theory, the Sinkhorn theorem.
Theorem (Sinkhorn). Let A be an N dimensional square matrix with positive entries. Then, there
exists two diagonal matrices D1 , D2 , with positive diagonals, so that P = D1AD2 is a doubly
stochastic matrix. These D1 , D2 are unique up to a scalar factor. Also, P can be obtained through
the iterative process of alternatively normalizing the rows and columns of A.

Proof. See Sinkhorn (1964); Sinkhorn & Knopp (1967); Knight (2008).
For our purposes, it is useful to deﬁne the Sinkhorn operator S (·) as follows:
Deﬁnition 1. Let X be an arbitrary matrix with dimension N . Denote Tr (X ) = X (cid:11)
N X ) (with (cid:11) representing the element-wise division and 1n the n
dimensional vector of ones) the row and column-wise normalization operators, respectively. Then,
we deﬁne the Sinkhorn operator applied to X ; S (X ), as follows:

(X 1N 1(cid:62)
N ), Tc (X ) = X (cid:11) (1N 1(cid:62)

(cid:0)Tr (S l−1 (X ))(cid:1) ,
S 0 (X ) = exp(X ),
S l (X ) = Tc
S (X ) = lim
n→∞ S l (X ).

Here, the exp(·) operator is interpreted as the component-wise exponential. By Sinkhorn’s theorem,
S (X ) is a doubly stochastic matrix.

Finally, we review some key properties related to the space of doubly stochastic matrices. First, we
need to deﬁne a relevant geometric object.
Deﬁnition 2. We denote by BN the N -Birkhoff polytope, i.e., the set of doubly stochastic matrices
of dimension N . Likewise, we denote Pn be the set of permutation matrices of size N . Alternatively,

BN = {P ∈ [0, 1] ∈ RN ,N P 1N = 1N , P (cid:62)1N = 1N },
PN = {P ∈ {0, 1} ∈ RN ,N P 1N = 1N , P (cid:62)1N = 1N }.

Theorem (Birkhoff). PN is the set of extremal points of BN . In other words, the convex hull of BN
equals PN .

Proof. See Birkhoff (1946).

A .1 AN A P PROX IMAT ION TH EOR EM FOR TH E MATCH ING PROB LEM

Let’s now focus on the standard combinatorial assignment (or matching) problem, for an arbitrary
norm) in the space of permutation matrices. In this context, let’s deﬁne the matching operator M (·)
N dimensional matrix X . We aim to maximize a linear functional (in the sense of the Frobenius
as the one that returns the solution of the assignment problem:

M (X ) ≡ arg max

(cid:104)P , X (cid:105)F .

P ∈PN
Likewise, we deﬁne ˜M (·) as a related operator, but changing the feasible space by the Birkhoff
polytope:

P ∈BN
Notice that in general ˜M (X ), M (X ) might not be unique matrices, but a face of the Birkhoff poly-
tope, or a set of permutations, respectively (see Lemma 2 for details). In any case, the relation

˜M (X ) ≡ arg max

(cid:104)P , X (cid:105)F .

(8)

(9)

15

Published as a conference paper at ICLR 2018

M (X ) ⊆ ˜M (X ) holds by virtue of Birkhoff ’s theorem, and the fundamental theorem of linear
programming.
Theorem 1. For a doubly stochastic matrix P deﬁne its entropy as h(P ) = − (cid:80)
Now we state the main theorem of this work:
Then, one has,

i,j Pi,j log (Pi,j ).

S (X/τ ) = arg max

(cid:104)P , X (cid:105)F + τ h(P ).

(10)

P ∈BN
continuous with respect to the Lebesgue measure in R. Then, almost surely the following conver-
Now, assume also the entries of X are drawn independently from a distribution that is absolutely
gence holds:

M (X ) = lim

τ →0+

S (X/τ ).

(11)

We divide the proof of Theorem 1 in three steps. First, in Lemma 1 we state a relation between
S (X/τ ) and the entropy regularized problem in equation (10). Then, in Lemma 2 we show that
under our stochastic regime, uniqueness of solutions holds. Finally, in Lemma 3 we show that in
this well-behaved regime, convergence of solutions holds. states that and Lemma 2b endows us with
the tools to make a limit argument.

A .1 .1

IN TERM ED IAT E R E SULT S FOR TH EOR EM 1

Lemma 1.

S (X/τ ) = arg max

P ∈BN

(cid:104)P , X (cid:105)F + τ h(P ).

Proof. We ﬁrst notice that the solution Pτ of the above problem exists, and it is unique. This is a
simple consequence of the strict concavity of the objective (recall the entropy is strictly concave Rao
(1984)).
Now, let’s state the Lagrangian of this constrained problem

L(α, β , P ) = (cid:104)P , X (cid:105)F + τ h(P ) + α(cid:62) (P 1N − 1N ) + β(cid:62) (P (cid:62)1N − 1N ),

It is easy to see, by stating the equality ∂L/∂P = 0 that one must have for each i, j ,

τ = exp(αi/τ − 1/2) exp(Xi,j /τ ) exp(βj /τ − 1/2),

pi,j

in other words, Pτ = D1 exp(Xi,j /τ )D2 for certain diagonal matrices D1 , D2 , with positive di-
agonals. By Sinkhorn’s theorem, and our deﬁnition of the Sinkhorn operator, we must have that

S (X/τ ) = Pτ .

Lemma 2. Suppose the entries of X are drawn independently from a distribution that is absolutely
continuous with respect to the Lebesgue measure in R. Then, almost surely, ˜M (X ) = M (X ) is a
unique permutation matrix.

Proof. This is a known result from sensibility analysis on linear programming which we prove for
completeness. Notice ﬁrst that the problem in (2) is a linear program on a polytope. As such, by
the fundamental theorem of linear program, the optimal solution set must correspond to a face of
the polytope. Let F be a face of BN of dimension ≥ 1, and take P1 , P2 ∈ F , P1 (cid:54)= P2 . If F is
an optimal face for a certain XF , then XF ∈ {X : (cid:104)P1 , X (cid:105)F = (cid:104)P2 , X (cid:105)F }. Nonetheless, the
latter set does not have full dimension, and consequently has measure zero, given our distributional
assumption on X . Repeating the argument for every face of dimension ≥ 1 and taking a union
bound we conclude that, almost surely, the optimal solution lies on a face of dimension 0, i.e, a
vertex. From here uniqueness follows.

Lemma 3. Call Pτ the solution to the problem in equation 10, i.e. Pτ = Pτ (X ) = S (X/τ ). Under
the assumptions of Lemma 2, Pτ → P0 when if τ → 0+ .

16

Published as a conference paper at ICLR 2018

Proof. Proof Notice that by Lemmas 1 and 2, Pτ is well deﬁned and unique for each τ ≥ 0.
Moreover, at τ = 0, P0 = M (X ) is the unique solution of a linear program. Now, let’s deﬁne
fτ (·) = (cid:104)·, X (cid:105)F + τ h(·). We observe that f0 (Pτ ) → f0 (P0 ). Indeed, one has:

f0 (P0 ) − f0 (Pτ ) = (cid:104)P0 , X (cid:105)F − (cid:104)Pτ , X (cid:105)F
= (cid:104)P0 , X (cid:105)F − fτ (Pτ ) + τ h(Pτ )
< (cid:104)P0 , X (cid:105)F − fτ (P0 ) + τ h(Pτ )
< τ (h(Pτ ) − h(P0 ))
< τ max
h(P ).

P ∈BN

From which convergence follows trivially. Moreover, in this case convergence of the values implies
the converge of Pτ : suppose Pτ does not converge to P0 . Then, there would exist a certain δ and
sequence τn → 0 such that (cid:107)Pτn − P0(cid:107) > δ . On the other hand, since P0 is the unique maximizer
of an LP, there exists ε > 0 such that f0 (P0 ) − f0 (P ) > ε whenever (cid:107)P − P0(cid:107) > δ , P ∈ BN . This
contradicts the convergence of f0 (Pτn ).

A .1 .2 PROO F O F TH EOR EM 1

The ﬁrst statement is Lemma 1. Convergence (equation 11) is a direct consequence of Lemma 3,
after noticing Pτ = S (X/τ ) and P0 = M (X ). We note that an alternative approach for the limiting
argument is presented in Cominetti & San Mart´ın (1994).

A .2 R E LAT ION TO SO F TMAX

Finally, we notice that all of the above results can be understood as a generalization of the well-
known approximation result arg maxi xi = limτ →0+ sof tmax(x/τ ). To see this, treat a category
as a one-hot vector. Then, one has

arg max

i

xi = arg max

e∈SN

(cid:104)e, x(cid:105),

(12)

where Sn is the probability simplex, the convex hull of the one-hot vectors (denoted Hn ). Again, by
the fundamental theorem of linear algebra, the following holds:

arg max

i

xi = arg max

e∈HN

(cid:104)e, x(cid:105).

(13)

On the other hand, by a similar (but simpler) argument than of the proof of theorem 4 one can easily
show that

sof tmax(x/τ ) ≡

(cid:80)

exp(x/τ )
i=1 exp(xi /τ )

(cid:104)e, x(cid:105) + τ h(e),

= arg max

e∈Sn

(14)

where the entropy h(·) is not deﬁned as h(e) = − (cid:80)n
A .3

I LLU STRAT ING TH EOR EM 1

i=1 ei log(ei )

17

Published as a conference paper at ICLR 2018

Figure 3: Illustrating the Matching and Sinkhorn operators, and the Gumbel-Matching and Gumbel-
Sinkhorn distributions. Each 5x5 grid represents a matrix, with the shading indicating cell values
(a) Matching operator M (X ) applied to a parameter matrix X . (b) Sinkhorn Operator S (X/τ )
approximating M (X ) for different temperature τ and number of Sinkhorn iterations, L. (c). First
row: samples from the Matching Sinkhorn distribution. Second and third rows: samples from
the Gumbel-Sinkhorn distribution at two temperatures. At low temperature, both distributions are
indistinguishable.

B SU P P LEM EN TA L M ETHOD S

B .1 EX PER IM EN TA L PROTOCO L S

All experiments were run on a cluster using Tensorﬂow Abadi et al. (2016), using several GPU
(Tesla K20, K40, K80 and P100) in parallel to enable an efﬁcient exploration of the hyperparameter
space: temperature, learning rate, and neural network parameters (dimensions).
In all cases, we used L = 20 Sinkhorn Operator Iterations, and a 10x10 batch size: for each sample
in the batch we used Gumbel perturbations to generate 10 different reconstructions.
For evaluation, we used the Hungarian Algorithm Munkres (1957) to compute M (X ) required to
infer the predicted matching.
Finally, experiments of section 5.4 were done consistent with model speciﬁcations stated in Linder-
man et al. (2017)

B .2 NUMB ER O F PARAM ET ER S ON S INKHORN N E TWORK S

In the simplest network, the one that sorts number, the number of parameters is given by nu+N ×nu :
Indeed, each number is connected with the hidden layer with nu (here, 32) units. This layer connects
with another layer with N units, representing a row of g( ˜X , θ).
For images, the ﬁrst layer is a convolution, composed by nf convolutional ﬁlters of receptive ﬁeld
size Ks with nc channels (one or three) followed by a ReLU + max-pooling (with stride s) opera-
s × nc × nf + nf . The second
tions. Then, the number of parameters in the ﬁrst layer is given by K 2
layers connects the output of a convolution, i.e., the stacked convolved l × l images by each of the
ﬁlters (after max-pooling) and p2 units, where p is the number of pieces each side was divided by.
Therefore, the number of parameters is given by l2 /(p2 s2 ) × nf × p2 = l2/s2 × nf , up to rounding
and padding subtleties. Then, the total number of parameters is l2 /s2 × nf + K 2
For the 3x3 puzzle on Imagenet, l = 256, p = 3, nc = 3 and the optimal network was such that
nf = 64, s = 2, Ks = 5. Then, it had 1,053,440 parameters.
Finally, for arbitrary assembly experiments, as one includes additional fully connected second layers,
the total number of parameters is nl × l2 /s2 × nf + K 2
s × nc × nf + nf , where nl is the number
of labels (here, nl = 10).

s × nc × nf + nf .

18

Sample 1Sample 2Sample 3Sample 4Sample 5(a)(b)(c)Published as a conference paper at ICLR 2018

IN FERENCE W I TH TH E IM PL IC I T GUMB EL -S INKHORN D I STR IBU T ION

B .3
Here we show how to compute K L((X + ε)/τ (cid:107) ε/τprior ), as deﬁned in 4.1. We ﬁrst notice that
the density of the variable h = (a + g)/b, where g has a Gumbel distribution and a, b are constants
is given by:

(15)
Therefore, the log density ratio LR(z ) between each component of h1 = (xi,j + εi,j )/τ and h2 =
εi,j /τprior is (suppressing indexing for simplicity)

log ph (z ) = log b − (bz − a + exp (a − bz )) .

LR(z ) = log ph1 (z )/ log ph2 (z )
= log τ − (τ z − x + exp (x − z τ )) − log τprior + (τprior z + exp (−z τprior )).

We need to take expectations with respect to the distribution of h1 . To compute this expectation, we
ﬁrst express the above ratio in terms of ε

LR(ε) = log(τ /τprior ) − (ε + exp (−ε) − (ε + x)τprior /τ − exp (−(ε + x)τprior /τ )))

Now we appeal to the law of the unconscious statistician, and take the expectation with respect to ε.
Using the identities
• E (ε) = γ ≈ 0.5772 (the Euler-Mascheroni constant)
• Moment generating function E (exp(tε)) = Γ(1 − t); implying E (exp(−ε)) = 1 and

E (exp (−τprior /τ ε)) = Γ(1 + τprior /τ ))

we have:

Eh1 (LR(z )) =Eε (LR(ε))
= log(τ /τprior ) − (γ (1 − τprior /τ ) + 1 − xτprior /τ − exp (−xτprior /τ ) Γ(1 + τprior /τ )).

From this, it easily follows (adding all the N 2 components) that

(cid:88)

K L((X + ε)/τ (cid:107) ε/τprior ) =
Eg1 (LR(zi,j ))
where S1 = τprior /τ (cid:80)
i,j xi,j and S2 = (cid:80)
=N 2 (log(τ /τprior ) − 1 + γ (τprior /τ − 1)) + S1 + Γ(1 + τprior /τ )S2 ,
i,j exp (−xi,j τprior /τ ).

i,j

C SU P P LEM EN TA L R E SU LT S

C .1 PU ZZ LE S

In table 4 we provide further performance measures for the Jigsaw puzzle task on Celeba, for extreme
hyper-parameter values: small temperature, large temperature, and a single Sinkhorn iteration These
are worse than the ones in table 2, although surprisingly, one Sinkhorn iteration already provides
reasonable performance, as long temperature is chosen in an appropriate range.

C .2 TRAN S FORMAT ION S IN TO ARB I TRARY D IG I T S

In table 5 we show performance of a 2-layer CNN in detecting transformed digits as the ones they are
intended to be. From this we see the most troublesome transformation was to one, as this network
most of the times categorized it as a different number. Also, in ﬁgure 4 we show transformations,
showing that to reconstruct to arbitrary digits it is not required that the original ones have an actual
digit-like structure, but they can be only pieces of ‘strokes’ or ‘dust’.

19

Published as a conference paper at ICLR 2018

Table 4: Jigsaw puzzle results for different extreme hyper-parameter values

Prop. wrong
Prop. any wrong
Kendall tau

l1
l2

t
i

g

i

d

l

a

u

t

c

A

0
1.
.91
1.
.04
1.
1.
.3
.0
1.
1.

1
.0
1.
.0
.0
.46
.0
.01
.73
.07
.33

0
1
2
3
4
5
6
7
8
9

2x2
.06
.1
.9
.03
.16

2
1.
.97
1.
1.
1.
1.
1.
.27
1.
1.

τ = 0.01

τ = 100

L = 1

3x3
.08
.22
.89
.04
.18

4x4
.23
.36
.74
.1
.28

5x5
.36
.9
.62
.14
.34

2x2
.03
.04
.97
.01
.11

3x3
.1
.23
.88
.04
.19

4x4
.28
.67
.7
.11
.3

5x5
.5
.97
.47
.19
.38

.2x2
.0
.0
1.0
.0
.0

3x3
.03
.08
.96
.01
.11

4x4
.13
.42
.86
.05
.21

5x5
.28
.82
.72
.11
.3

Becomes
3
4
1.
1.
.99
.99
1.
1.
1.
1.
1.
1.
1.
.63
1.
1.
.46
1.
1.
1.
1.
1.

5
1.
1.
1.
1.
1.
1.
1.
1.
1.
1.

6
1.
1.
1.
.96
1.
1.
.65
1.
1.
1.

7
1.
.56
.70
1.
1.
1.
1.
1.
.07
1.

8
1.
.75
1.
1.
.68
1.
.65
1.
1.
1.

9
1.
.2
1.
.96
.36
1.
1.
.72
1.
.66

Table 5: Accuracies of two-layer convolutional neural network in identifying transformed digits

C .3 R E SU LT S ON CATEGOR IA L VAE IN MN IST

In general, for arbitrary random variables Z1 , Z2 and a function g , one has

K L(Z1 (cid:107) Z2 ) ≥ K L(g(Z1 ) (cid:107) g(Z2 )).

(16)

We prove this in the discrete case, for simplicity: call q(z ) and p(z ) the densities of Z1 , Z2 , and call
y = g(z ). This induces two joint distributions, p(z , y) and q(z , y). Now, deﬁne

(cid:88)

K L(q(z |y) (cid:107) p(z |y)) =

(q(z , y) log q(z |y) − log p(z |y)).

Under this deﬁnition, one can verify that

y ,z

K L(q(z , y) (cid:107) p(z , y)) =K L(q(z ) (cid:107) p(z )) + K L(q(y |z ) (cid:107) p(y |z ))
=K L(q(y) (cid:107) p(y)) + K L(q(z |y) (cid:107) p(z |y)).

(cid:107)

Therefore,

But K L((q(y |z )
K L((q(z ) (cid:107) p(z )) = K L(q(y) (cid:107) p(y)) + K L(q(z |y) (cid:107) p(z |y)), and since the second term is

p(y |z )) = 0, as y is a deterministic function of z .
positive (a KL divergence) we conclude K L(q(z ) (cid:107) p(z )) ≥ K L(q(y) (cid:107) p(y)).
This implies a lower (or less tight) ELBO if using Z1 , Z2 instead of g(Z1 ), g(Z2 ). However, we note
that in the categorical case this has a minimal impact in performance. Indeed, we replicated the den-
sity estimation on MNIST task described in Jang et al. (2016); Maddison et al. (2016), and as alter-
native method we considered the concrete distribution, but using as stochastic node (ε + x)/τ (with
prior ε/τprior instead of two concrete distributions. In other words, for us g(x) = softmaxτ (x)
and Z1 = (ε + x)/τ , Z2 = (ε)/τprior (in law). Results are shown in Table 6. We ﬁrst see that
Concrete distribution does worse than Gumbel-Softmax, which we attribute to a sub-optimal param-
eter search. However, we see that working in the Gumbel space has little impact on log p(x): the
difference was smaller than .5 nats.

20

Published as a conference paper at ICLR 2018

Figure 4: First column: samples from dataset created by mixing all pieces of digits, and then re-
assembling them into ‘digits’. Second column: random permutations of ﬁrst column. Third column:
hard reconstructions using M (X ). Fourth column: soft reconstructions using S (X/τ ) and τ = 1.
Metaphorically, one is able to reconstruct pieces out of ‘dust’.

Method

Gumbel-Softmax
Concrete
Concrete (Gumbel space)

− log p(x)

106.7
111.5
111.9

Table 6: Summary of results in VAE

Mean number of candidates
Difﬁculty
MCMC
(Linderman et al., 2017)
Gumbel-Sinkhorn
Gumbel-Sinkhorn
(no regularization)

10
1 worm 4 worms
.34
.65
.77
.93

.79

.94

0.77

.92

30
1 Worm 4 worms
.18
.28
.33

.7

.4
.4

.69
.64

45
1 worm 4 worms
.14
.17
.18
.48

.25

.51

60

1 worms
.13
.17

.21

4 worms
.16
.37

.44

.25

.44

.21

.39

Table 7: Accuracy in the C.elegans neural identiﬁcation problem, for varying mean number of
candidate neurons (10, 30, 45, 60) and number of worms (1 and 4).

C .4 SU P P LEM EN TARY R E SULT S ON C . E LEGAN S

Finally, in Table 7 we show additional results for the C.elegans experiment. The setting is the same
as in Figure 4(a) in Linderman et al. (2017). Likewise, Table 3 correspond to the setting of Figure
4(b) in Linderman et al. (2017).

D SU P P LEM EN TARY D I SCU S S ION

D .1 S INKHORN O PERATOR FOR A P PROX IMAT E MARG INA L IN F ER ENC E

A second connection between the distribution in (6) (and therefore, the Matching Gumbel distribu-
tion) and the Sinkhorn operator arises as a consequence of Theorem 1. This relates to the estimation
of the marginals Eθ (Pi,j ), known to be a #P hard problem. A well known result (Globerson &
Jaakkola, 2007; Wainwright et al., 2008), consequence of Fenchel (conjugate) duality (Rockafellar,
1970) applied to exponential families, links this problem to optimization in the following way: lets
denote by M the marginal polytope, the convex hull of the set of realizable sufﬁcient statistics, that
here coincides with Bn . Also, lets call H(µ) the entropy of (6) for the parameter θ(µ) such that

21

Hard reconstructionsSoftReconstructionsMixedScrambledPublished as a conference paper at ICLR 2018

µ = Eθ(µ) (P ). Then,

µ∈M(cid:104)θ , µ(cid:105)F + H(µ).
Eθ (P ) = arg max

(17)

Notice the only difference between the optimization problems in (17) and (10) is the entropy term,
after identifying X with θ . Therefore, one may understand the Sinkhorn operator as providing
approximations for the partition function and the marginals, which will be accurate insofar as h(µ)
is a good approximation for H(µ). In this way, one can understand S (X ) as an approximation for
Eθ (P ), that may complement more classical ones, as the Bethe and Kituchani’s approximations for
H(µ), and the corresponding approximate inference algorithms that they give rise to (Yedidia et al.,
2001; Vilnis et al., 2015).

D .2 SUMMARY O F EX TEN S ION S

Table 8: Analogies between permutation and categories

Polytope

Linear program

Approximation

Entropy
Entropy regularized
linear program

Reparameterization

Continuous
approximation

Categories

Permutations

Probability simplex S

Birkhoff polytope BN

arg max xi = arg maxs∈S (cid:104)x, s(cid:105)
M (X ) = arg maxP ∈B (cid:104)P , X (cid:105)F
h(s) = (cid:80)
arg maxi xi = limτ →0+ softmax(x/τ )
h(P ) = (cid:80)
M (X ) = limτ →0+ S (X/τ )
i −si log si
softmax(x/τ ) = arg maxs∈S (cid:104)x, s(cid:105) + τ h(s) S (X/τ ) = arg maxP ∈B (cid:104)P , X (cid:105)F + τ h(P )

i,j −Pi,j log (Pi,j )

Gumbel-max trick

arg maxi (xi + i )

Concrete

softmax((x + )/τ )

Gumbel-Matching GM (X )
Gumbel-Sinkhorn GS (X, τ )

M (X + )

S ((X + )/τ )

22

