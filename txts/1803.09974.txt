8
1
0
2

r

a

M

9
2

]

C

N

.

o

i

b

-

q

[

2
v
4
7
9
9
0

.

3
0
8
1

:

v

i

X

r

a

Inferring network connectivity from event timing patterns

Jose Casadiego1,2, Dimitra Maoutsa2 , Marc Timme1,2,3,4

1Chair for Network Dynamics, Institute of Theoretical Physics and Center for Advancing Electronics Dresden (cfaed),
Technical University of Dresden, 01062 Dresden, Germany
2Network Dynamics, Max Planck Institute for Dynamics and Self-Organization (MPIDS), 37077 Göttingen, Germany
3Bernstein Center for Computational Neuroscience (BCCN), 37077 Göttingen, Germany
4Advanced Study Group, Max Planck Institute for the Physics of Complex Systems, 01069 Dresden, Germany

Reconstructing network connectivity from the collective dynamics of a system typically requires
access to its complete continuous-time evolution although these are often experimentally inaccessible.
Here we propose a theory for revealing physical connectivity of networked systems only from the
event time series their intrinsic collective dynamics generate. Representing the patterns of event
timings in an event space spanned by inter-event and cross-event intervals, we reveal which other
units directly inﬂuence the inter-event times of any given unit. For illustration, we linearize an
event space mapping constructed from the spiking patterns in model neural circuits to reveal the
presence or absence of synapses between any pair of neurons as well as whether the coupling acts
in an inhibiting or activating (excitatory) manner. The proposed model-independent reconstruction
theory is scalable to larger networks and may thus play an important role in the reconstruction of
networks from biology to social science and engineering.

The topology of interactions among the units of net-
work dynamical systems fundamentally underlies their
systemic function. Current approaches to reveal interac-
tion patterns of a network from the collective nonlinear
dynamics it generates [1–9] rely on directly sampling the
trajectories of the collective time evolution. Such sam-
pling requires experimental access to the continuously
ongoing dynamics of the network units.

For a range of systems, however, direct access to the
units’ internal states is not granted, but only times of
events are available. Prominent examples include the
times of messages initiated or forwarded in online so-
cial networks and distributed patterns of action poten-
tials (spikes) emitted by the neurons of brain circuits,
both reﬂecting the respective network structure in a non-
trivial way [10–15]. Reconstruction of physical network
connectivity from such timing information has been at-
tempted for speciﬁc settings for neural circuits or on-
line social contacts. Often it is limited to small net-
works (102 units), by large computational eﬀorts includ-
ing high-performance parallel computers up to about 103
units, or to knowing speciﬁc system models in advance
[13, 14, 16–20]. For instance, recent eﬀorts on recon-
structing spiking neural circuit connectivity [19] show
that combining stochastic mechanisms for spike gener-
ation and linear kernels for spike integration enables re-
construction of larger networks (103 neurons), if the spike
integration model closely matches the original simulated
systems. Alternatively, in systems of pulse-coupled units
[20] reconstruction is feasible if the units are all intrin-
sic oscillators. Besides such speciﬁc solutions, a general
model-independent theory based on timing information
generated by the collective network dynamics is as yet
unknown.

In this Letter, we propose a general theory for recon-
structing physical network connectivity based only on the

b

a

Neuron 1
Neuron 1

Neuron 2

Neuron 3

Neuron N

Figure 1. Representing timing patterns in event space.

a, Schematics of an event for a sample unit (i = 1) formed by
the inter-event time of unit 1 and all cross-event intervals that
may inﬂuence the subsequent event of unit 1. An additional
counting index m is dropped for simplicity b, In the event
space for neuron i, each event ei,m is represented by a point
on the manifold deﬁned through (3).

event timing patterns generated by the collective spik-
ing dynamics. The theory reveals existence and absence
of interactions, their activating or deactivating nature,
and enables reliable network reconstruction from regular
as well as irregular timing patterns, even if some (hid-
den) units cannot be observed. The proposed reconstruc-
tion theory is model-independent (thereby, purely data-
driven) and trivially parallelizable because linearized
mappings for diﬀerent units are computationally inde-
pendent of each other.

Mapping timing patterns to physical connections? To

present the proposed theory consistently, we focus on a
setting (and notation) of networks of spiking neurons.
Alternate applications work in qualitatively the same way
and are discussed towards the end of this article. Here
the units are individual neurons, a physical connection
is a synapse from one neuron to another and the events
observed for each unit are the electrical action potentials

 
 
 
 
 
 
or spikes emitted by the neuron. Speciﬁcally, inter-event
intervals are inter-spike intervals (ISIs) and cross-event
intervals cross-spike intervals (CSIs). The theory stays
unchanged (up to notation) for all kinds of event times
observed from and originally generated by any speciﬁc
collective network dynamics that are typically unknown
but coordinated via the network interactions.
Thus, consider a network of N units i ∈ {1, . . . , N }
generating spatio-temporal spike patterns (Fig. 1) de-
ﬁned by the sets of times ti,m , where m ∈ N counts the
spike times. An inter-spike interval (ISI)

∆Ti,m := ti,m − ti,m−1 > 0,

(1)

measures the duration of time between two consecutive
events, the (m − 1)-st and the m-th spike times ti,m−1
and ti,m of neuron i. Similarly, the cross-spike intervals
(CSIs)

wi

j,k,m := tj,p − ti,m−1 > 0,

(2)

measure the duration between the p-th spike generated
by neuron j and the previous ((m − 1)st) spike generated
by i with tj,p < ti,m (all superscripts throughout this
article denote indices, not powers). We index the CSIs
by integers k ≡ k(ti,m−1 , tj,p ) starting with k = 1 for the
ﬁrst spike at tj,p = minp′ {tj,p′ | tj,p′ > ti,m−1} of unit
j after ti,m−1 and sequentially increasing k by one by
counting through the sequence of tj,p forwards in time.
Figure 1 illustrates this deﬁnition for one given ISI, where
the indices m − 1 and m are suppressed. Now consider
that some ﬁnite number K i of spike times (for each of
the other neurons j ) preceding ti,m inﬂuences the time
ti,m in a relevant way.
As a core conceptual step, we propose that the ISIs of
each neuron i are approximately given by some unknown,
locally smooth function hi : RN ×K i
→ R of the K i rele-
vant cross-spike intervals (see Figure 1b) such that
∆Ti,m = hi (cid:0)ΛiW i
for all m. Here the explicit dependency matrix Λi ∈
{0, 1}N ×N , cf.
[21], is a diagonal matrix (to be deter-
mined) indicating whether there is a physically active
(synaptic) connection from neuron j to i (Λi
jj = 1) or
not (Λi
jj = 0). The matrix
m := 
W i

1,2,m . . . wi
2,2,m . . . wi

1,1,m wi
2,1,m wi

∈ RN ×K i

m (cid:1)

wi
wi

1,K i ,m

2,K i ,m

(3)

,

...

. . .

...




...



wi

N ,1,m wi

N ,2,m . . . wi

N ,K i ,m

(4)
collects the K i CSIs generated by each neuron j until
just before the m-th spike generated by neuron i. We
refer to the k-th column
k,m := (cid:2)wi

N ,k,m (cid:3)

2,k,m , . . . , wi

1,k,m , wi

∈ RN ,

(5)

w

T

i

2

of W i
m as the k-th presynaptic proﬁle of neuron i be-
fore its m-th spike time. It indicates when presynaptic
neurons ﬁred for the ﬁrst time, second time, and so on
until K i -th time, before ti,m . For illustration purposes,
we here speciﬁcally constrain K i to take into account
only those spike times within the currently considered
ISI such that tj,p ∈ [ti,m−1 , ti,m ]. If a presynaptic neuron
does not spike within this interval, its CSIs are set to
zero, wi
j,k,m := 0, in (4). From a biological perspective,
the function hi is determined by the intrinsic properties
of neuron i including its spike generation mechanism as
well as its pre- and postsynaptic processes. The function
hi is in general unknown.
In particular, equation (3) assigns a speciﬁc ISI to
a speciﬁc collection of timings of presynaptic inputs.
Therefore, we may represent such ISI-CSIs tuple in a
higher dimensional event space Ei ⊂ R(N K i+1) , where
each realization of neuron i’s dynamics is given by events
deﬁned as
ei,m := (cid:2)vec (cid:0)W i
m (cid:1) , ∆Ti,m (cid:3)
where the operator vec : RS×R → RSR stands for vec-
torization of a matrix and it transforms a matrix into a
column vector [22].
So, how can events ei,m in the event space Ei help us
determine the synaptic inputs to neuron i? Consider a
local sample of M + 1 events in Ei as follows (Figure 2a).
Selecting a reference event

∈ R(N K i+1) ,

(6)

T

ei,r := arg min

ei,s Xm

kei,m − ei,sk2 ,

(7)

closest to all other events in the sample, with m, s ∈
{1, 2 . . . , M + 1}, yields an approximation

∆Ti,m

= ∆Ti,r + tr (cid:18) ∂ hi
.
∂W i (cid:19)T
Λi (cid:2)W i
m − W i
of (3) linear in the diﬀerences (cid:2)W i
m − W i
r (cid:3) around ei,r .
Here (cid:0)∂ hi/∂W i(cid:1) ≡ (cid:0)∂ hi /∂W i(cid:1) (cid:0)Λi (cid:0)W i
is a
matrix derivative and tr(·) is the trace operation. Rewrit-
ing (8) yields

r (cid:1)(cid:1) ∈ RN ×K i

r (cid:3)! ,

(8)

∆Ti,m

.
= ∆Ti,r +

Ki

Xk=1

∇hi,kΛi (cid:2)w

k,m − w
i

k,r (cid:3) ,
i

(9)

,

2k

1k

, . . . ,

where ∇hi,k := (cid:20) ∂ hi
∂ hi
∂ hi
N k (cid:21) ∈ RN is the
∂W i
∂W i
∂W i
gradient of the function at the k-th presynaptic proﬁle.
Thus, given m ∈ {1, 2, . . . , M } diﬀerent events (in addi-
tion to the reference event r), ﬁnding the synaptic con-
nectivity becomes solving the linear regression problem
(9) for the unknown parameters ∇hi,kΛi .
In particu-
lar, system (9) may be computationally solved in paral-
lel for diﬀerent units i. We here solve such linear system

a

b

inhibitory

excitatory

a

Regular

b

Intermediate

Irregular

3

Figure 2. Slopes in event space yield excitatory and in-
hibitory synaptic interactions. a, Schematics of a local

sampling in the event space. Local samplings may be taken
from regular spiking patterns or subsets of irregular spike pat-
terns such that events are close-by in event space. Taking a
reference event (red dot) and linearly approximating all other
events (gray dot) through (9) constrains existence and sign
of interactions without knowing a system model. b, Recon-
struction of inhibitory (green), excitatory (orange) and absent
(blue) synaptic interactions of a neuron in a random network
of N = 100 LIF neurons having δ -synapses with Nexc = 50 ex-
citatory and, Ninh = 50 inhibitory neurons and a connection
probability p = 0.1. See [26] for further parameters. The red-
dashed lines indicate the optimal thresholds (as calculated via
Otsu’s method [27]) to distinguish excitatory and inhibitory
from absent interactions.

Figure 3. Revealing synaptic connections from regular
to irregular spiking patterns. a, Schematic representa-

tion of spike trains with diﬀerent degrees of irregularity. To
establish test statistics, the spike trains were sampled from
blue random networks of N = 100 inhibitory LIF neurons in-
teracting via δ -synapses with connection probability p = 0.1.
All simulations were performed using identical time intervals
of 500 s each (see [26] for further settings). b, AUC scores for
two diﬀerent sampling conditions: random and closest. In the
random sampling paradigm, events are randomly drawn from
the uniform distribution across the spike trains, while in a
closest sampling paradigm, the same number of events closest
to a reference event are jointly considered for reconstruction.
Reconstruction quality decreases with spiking pattern irregu-
larity for random sampling (dark gray) yet stays consistently
high for closest sampling. Gray dashed line indicates random
guessing.

via least squares minimization, compare [8, 23]. Block-
sparse regression algorithms may also be employed, espe-
cially if one proposes higher-order approximations than
(9) [21, 24].

Revealing synapses in networks of spiking neurons.

To validate the predictive power of our theory, we re-
vealed the synaptic connectivity (i) of model systems
with current-based and conductance-based synapses, (ii)
of excitatory and inhibitory synapses, (iii) with both
instantaneous and temporally extended responses, (iv)
of moderately larger number of neurons (compared to
the state-of-the-art), (v) for regular and irregular spiking
patterns and (vi) under conditions where a subset of units
is hidden or unavailable to observation (Figures 2, 3, and
4). We quantify performance of reconstruction by the
Area Under the Receiver-Operating-Characteristic Curve
(AUC) score [25], that equals 1 for perfect reconstruction
and 1/2 for predictions as good as random guessing.
We start illustrating successful reconstruction for sim-
ple networks of leaky integrate-and-ﬁre (LIF) [28] neu-
rons with mixed inhibitory and excitatory current-based
δ -synapses (see Supplemental Material [26]). As Figure
2b shows by example, the method of event-space map-
pings does not only reveal the existence and absence of
synapses between pairs of neurons in the network;
in
particular, the signs and magnitudes of the derivatives
(cid:0)∂ hi/∂W i
j1 (cid:1) already indicate whether a synapse acts in
an inhibitory or excitatory way:
indeed, inhibitory in-
puts to neuron i retard its subsequent spike and thereby
extend the duration of an ISI, whereas excitatory inputs

shorten the duration. Thus, positive derivatives indicate
(eﬀectively) inhibitory and negative derivatives excita-
tory interactions, compare Figure 2b.

Reconstruction from irregular event patterns? What if

the spiking patterns are not regular as often the case for
real data (from neurophysiological recordings as well as
from observations in any other discipline) and thus not all
events are located suﬃciently close to one reference event
(7)? If the induced events ei,m are distributed in event
space Ei less locally, with the ei,m located on diﬀerent,
possibly non-adjacent patches in Ei , we systematically
collect only those events that are located close to selected
reference events ei,r [29]. To check performance, we sys-
tematically varied how irregular spike timing patterns are
by varying the overall coupling strength by a factor of 30,
eﬀectively interpolating between regimes close to regular
dynamics (locking) [30] for small coupling and close to ir-
regular balanced states [12, 31–33] for large coupling. If
suﬃciently many events (ISI-CSI combinations) that are
close in event space occur in the network’s spike timing
patterns, sampling the events in some local region (or,
alternatively, local regions) in event space may be com-
pensated by longer recording times or collecting several
patches of events. With increasing inhibition, the total
number of spikes in each recording decreases whereas the
spike sequence irregularity increases. Thereby, to ensure
a suﬃcient number of events locally in event space across
inhibition levels, we simulated all networks for 500 s each.
We indeed ﬁnd that such closest sampling paradigm en-
ables consistent high-quality reconstruction for regular,

a

c

b

d

Figure 4. Model-independence and robustness against

hidden units. Quality of reconstruction vs.
the num-
ber of events M considered for blue random networks of
N = 100 blue and p = 0.1 a, LIF neurons with α-synapses
(ESL, CCorr, MI, STA and RG stand for event-space lin-
earization (our approach, marked in blue), cross-correlations
(green), mutual information (orange), spike-triggered average
(rose) and random guessing (gray), respectively); b, Hodgkin-
Huxley neurons with conductance-based synapses; c, LIF
neurons coupled with δ -synapses where only 80 neurons are
observed. d, Systematic reconstructions versus the fraction
of observed neurons. Network parameters are Nexc = 50 and
Ninh = 50, see [26] for further settings.

intermediate and irregular spike timing patterns alike
(Fig. 3).

We reiterate that approximation (9) requires no a pri-
ori information about circuit or neuron models and pa-
rameters, instead, only spike timing data are necessary
to reveal synapses. The same event-space/linearization
(ESL) method proposed performs robustly across diﬀer-
ent circuits of neurons (low- and higher-dimensional neu-
ron models and diﬀerent synaptic models), compares fa-
vorably to alternative approaches of model-free recon-
struction and even yields reasonable estimates if some
units are hidden (see [26] for more details), see Fig-
ure 4. Figure panels 4a and b illustrate the perfor-
mance for networks of simple LIF neurons with current-
based synapses as well as for more biophysically detailed
Hodgkin-Huxley neurons coupled via conductance-based
synapses (see [26]) explicating that the theory is insen-
sitive to changes of types of coupling and neurons. The
ESL method outperforms predictions by statistical de-
pendencies such as cross-correlations, mutual informa-
tion and spike-triggered averages. In addition, if some
neurons are inaccessible (hidden units), synapses among

4

a

b

c

Figure 5. Reconstructing large networks is computa-

tionally feasible. Reconstruction of random networks of
N = 2000 LIF neurons with δ -synapses and p = 0.1. See [26]
for more details. a, Connectivity matrix excitatory (red) and
inhibitory (blue) synapses. Inset shows a close up. b, Dis-
tribution of reconstructed excitatory (orange), absent (blue)
and inhibitory (green) synapses of a single postsynaptic neu-
ron. c, Quality of reconstruction for individual neurons for
M = 8000. ESL, CCorr, MI and STA stand for the ap-
proaches introduced here, using cross-correlations, mutual
information and spike-triggered averages, respectively. The
gray-dashed line stands for random guessing.

accessible neurons may still be recovered (see [26]), Fig-
ure 4c-d. Furthermore, a systematic study shows that
ESL accurately determines synaptic links in the presence
of external inputs emulating neurons ﬁring with Poisson
statistics, at the expense of requiring to record a larger
number of events (see Supplementary Material [26] for a
detailed study). As explained above, recording a larger
number of events promotes denser samplings in the event
space, which in turn aids in ﬁltering out dynamical ef-
fects related to such unobserved inputs when solving the
regression problem.
Finally, reconstructing larger networks seems compu-
tationally feasible and reconstruction quality compares
favorably to other general, model-free approaches. For
instance, Figure 5 illustrates successful reconstruction of
the presynaptic pattern of a random unit i in a network of
N = 2000 neurons exhibiting regular dynamics that was
computed within about 500 seconds (∼ 10 minutes per
neuron) on a single machine [34]. Generally, the compu-
tational (time) complexity of our reconstruction theory
resutls from the computation of event-space distances,
which scales as O (cid:0)(M N K )2(cid:1) (see [26]). The approach
separates not only existing from absent but also exci-
tatory from inhibitory synapses. As before, a systematic
comparison for equal number of events shows that our ap-
proach again outperforms predictions by commonly em-
ployed statistical dependency quantiﬁers, Figure 5c.

Summary and Conclusions. We presented a general,

model-independent theory for reconstructing the topol-
ogy of a network’s direct interactions from observed pat-
terns of event timings only. A key advance is the proposal
that the interevent intervals are given by some unknown,

suﬃciently smooth function, thereby not requiring access
to a event generating system model or even the full state
vector of the dynamical system generating the events. We
illustrated core performance aspects including robustness
(against changes in unit dynamics and coupling schemes),
computational performance (rapid analysis on single ma-
chines and parallelizability) and suitability even if some
units are hidden by example of spiking neural networks.
By representing inter-event intervals as functions of cross-
event intervals, we mapped the problem to event spaces
yielding linear equations that enable robust least squares
solutions for the topology. Thereby, the approach may be
transferred to other systems generating event time series.
We remark that ∇hi,kΛi in (9) maps exactly those
physical connections that are used for transmitting sig-
nals that actually directly inﬂuence the timings ti,m and
thus the inter-event times. This has distinct conse-
quences in practice. For instance, a presynaptic neuron
j may generate its only potentially relevant spike dur-
ing the refractory period of neuron i (or may simply not
generate any spike) during the observation period of the
experiment recording the timing pattern, thus no synapse
from unit j will be indicated even if an anatomical one
exists. At the same time, if for any reason (e.g. measure-
ment error or data corruption) an event of unit i is not
recorded at all or the ISI recorded with a large error, it
becomes easy to spot this event as an outlier as it would
lie far above or below the other events with nearby CSIs.
As only the presence (and sign) of the above derivatives
play a role for reconstruction, a straightforward general-
ization is to collect events close to several references ei,r ,
ei,r ′ , etc. and concatenate local (and generally diﬀerent)
approximations of the form (9).
A recent work [35] studying excitatory-to-inhibitory
CA1 synapses in vivo focused on predicting exclusively
the strongest interactions from excitatory to inhibitory
neurons using a statistical GLM-based method com-
bined with cross-correlograms, which results in high-
computational demands when applied on large networks.
The clearest advantages of our theory, beyond its sim-
plicity, are its rapid computational performance and its
generality and model-independence, compare to a recent
alternative approach [20]. Moreover, our ansatz by con-
struction generalizes to systems beyond spiking neural
networks (used here as a systematic case study for illus-
tration) and may be applied, for instance, for revealing
friendship networks in online social networks [13, 14] or to
determine who communicates with whom in hidden web
or wireless services [36, 37] – from the timing of local
unit activities alone. Taken together, the results illus-
trated above suggest the power of systematically gener-
alizing a state space perspective for representing the full
trajectories to an event space perspective for representing
collectively coordinated event timing patterns.
Acknow ledgements. We thank Viola Priesemann, Juan
Florez-Weidinger, Mantas Gabrielaitis and Stayko Popov

5

for valuable discussions. We gratefully acknowledge sup-
port from the Federal Ministry of Education and Re-
search (BMBF Grant No. 03SF0472F), the Max Planck
Society and the German Research Foundation (DFG)
through the Cluster of Excellence Center for Advanc-
ing Electronics Dresden (cfaed). J. Casadiego and D.
Maoutsa contributed equally to this work.

L. Kocarev,

[1] M. K. Yeung,
J. Tegner,
and J. J. Collins,
Proc. Natl. Acad. Sci. U. S. A. 99, 6163 (2002).
[2] T. S. Gardner, D. di Bernardo, D. Lorenz, and J. J.
Collins, Science 301, 102 (2003).
[3] D. Yu, M. Righero,
and
Phys. Rev. Lett. 97, 188701 (2006).
[4] M. Timme, Phys. Rev. Lett. 98, 224101 (2007).
[5] D. Yu and U. Parlitz, Phys. Rev. E 82, 026108 (2010).
[6] W.-X. Wang, Y.-C. Lai,
and C. Grebogi,
Phys. Rep. 644, 1 (2016).
[7] B. Barzel and A. L. Barabási, Nat. Biotechnol. 31, 720
(2013).
[8] M.
Timme
and
J.
Casadiego,
J. Phys. A Math. Theor. 47, 343001 (2014), 1408.2963 .
[9] M. Nitzan,
J. Casadiego,
and M. Timme,
Sci. Adv. 3, e1600396 (2017).
[10] R. M. Memmesheimer and M. Timme, Physica D 224,
182 (2006).
[11] R. M. Memmesheimer and M. Timme, Phys. Rev. Lett.
97, 188101 (2006).
[12] S. Jahnke, R. M. Memmesheimer, and M. Timme, Phys.
Rev. Lett. 100, 048102 (2008).
[13] T. Aoki, T. Takaguchi, R. Kobayashi, and R. Lambiotte,
Phys. Rev. E 94, 042313 (2016).
[14] R.
Kobayashi
and
R.

Lambiotte,

in

Tenth International AAAI Conference on Web and Social Media

and D. Battaglia,

(2016).
[15] C. Kirst, M. Timme,
Nat. Commun. 7, 11061 (2016).
[16] V. A. Makarov, F. Panetsos,
and O. De Feo,
J. Neurosci. Methods 144, 265 (2005).
[17] F. Van Bussel, B. Kriener,
and M. Timme,
Front. Comput. Neurosci. 5, 3 (2011).
[18] F. Gerhard,
T. Kispersky,
G.
J. Gutier-
rez, E. Marder, M. Kramer,
and U. Eden,
PLoS Comput. Biol. 9, e1003138 (2013).
[19] Y. V. Zaytsev, A. Morrison, and M. Deger, J. Comput.
Neurosci. 39, 77 (2015).
[20] R.
Cestnik
and
Phys. Rev. E 96, 012209 (2017).
[21] J. Casadiego, M. Nitzan, S. Hallerberg, and M. Timme,
Nat. Commun. 8, 2192 (2017).
[22] J. Magnus and H. Neudecker, Technometrics , Vol. 31
(1989) p. 501.
[23] S.
G.
Shandilya
and
New J. Phys. 13, 013004 (2011).
[24] Y.
C.
Eldar
and
M.
IEEE Trans. Inf. Theory 55, 5302 (2009).
[25] T. Fawcett, Pattern Recognit. Lett. 27, 861 (2006).
[26] See Supplemental Material for more details on neuron
network models, simulation parameters and additional

Rosenblum,

Mishali,

M.

M.

Timme,

results.
[27] N. Otsu, IEEE Trans. Syst. Man. Cybern. 9, 62 (1979).
[28] N. Brunel, Comput. Neurosci. 8, 183 (2000).
[29] One may alternatively collect events close to several ref-
erences ei,r , ei,r ′ , etc. and concatenate local (and gen-
erally diﬀerent) approximations of the form (9) yielding
constraints of the same (linear) type and solve the regres-
sion problem via block-sparse regression [21].
[30] C. van Vreeswijk, Phys. Rev. E 54, 5522 (1996).
[31] C.
van
Vreeswijk
and
H.
Sompolinsky,
Science 274 (1996).
[32] M.
Timme,
F. Wolf,
Phys. Rev. Lett. 89, 258701 (2002).
[33] D. Hansel and G. Mato, J. Neurosci. 33, 133 (2013).

T. Geisel,

and

6

[34] Reconstructions were performed on a single
core
of a desktop computer with Intel® Core™ i7-4770
CPU@3.40GHz octocore processor and 16 GB of RAM
memory . Most of execution time is exhausted in the com-
putation of distances among the events. Reconstruction
time is further determined by the speed of the numerical
package chosen to perform least squares optimization.
[35] D. F. English, S. McKenzie, T. Evans, K. Kim, E. Yoon,
and G. Buzsáki, Neuron 96, 505 (2017).
[36] C.
Bettstetter
and
Christian,

in

Proc. 3rd ACM Int. Symp. Mob. ad hoc Netw. Comput. - MobiHoc ’02

(ACM Press, New York, New York, USA, 2002) p. 80.
[37] J. Klinglmayr, C. Kirst, C. Bettstetter, and M. Timme,
New J. Phys. 14, 073031 (2012).

