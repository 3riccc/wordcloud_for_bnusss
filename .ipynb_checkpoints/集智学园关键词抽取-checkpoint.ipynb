{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/pb/nkhgj9gx1_d0n442qmwrcdjc0000gn/T/jieba.cache\n",
      "Loading model cost 1.114 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import copy\n",
    "# import jieba\n",
    "# import re\n",
    "# import os\n",
    "# jieba.load_userdict('lexicon.txt')\n",
    "# jieba.load_userdict('lexicon_by_manual.txt')\n",
    "# jieba.load_userdict('keyword_1_py.txt')\n",
    "# jieba.load_userdict('topic.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_old_read = pd.read_excel('original_title.xlsx')\n",
    "# original_article_list = list(df_old_read['title'])\n",
    "# article_list = os.listdir('pages/')\n",
    "# print('length of the article set:', len(article_list))\n",
    "# new_article = []\n",
    "# for title in article_list:\n",
    "#     if title not in original_article_list:\n",
    "#         new_article.append(title)\n",
    "# print('there are',len(new_article),'new article(s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_stopword(filename):\n",
    "    '''\n",
    "    加载停止词\n",
    "    :return:\n",
    "    '''\n",
    "    with open(filename,'r',encoding='utf-8') as f:\n",
    "        stopword = [line.strip() for line in f]\n",
    "    print(stopword[0:10])\n",
    "    return stopword\n",
    "\n",
    "def extract(contents, stopword):\n",
    "    '''\n",
    "    用正则表达式去除无用内容以及停止词，保留有用的内容\n",
    "    :param contents: list\n",
    "    :param stopword: list\n",
    "    :return:\n",
    "    '''\n",
    "    extracted_contents = []\n",
    "    for content in contents:\n",
    "        extracted_words = []\n",
    "        content = str(content).strip()\n",
    "        re_h = re.compile('</?\\w+[^>]*>')\n",
    "        re_nbsp = re.compile('&nbsp;')\n",
    "        content = re_h.sub('', content)\n",
    "        content = re_nbsp.sub('', content)\n",
    "        words = jieba.lcut(content)\n",
    "        for w in words:\n",
    "            #\n",
    "            w = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\'“”《》?“]+|[+——！，。？、~@#￥%……&*（）：]+\", \"\", w)\n",
    "            if len(w)>0 and w not in stopword:\n",
    "                ## nltk 词性变化\n",
    "                extracted_words.append(w)\n",
    "        #if len(extracted_words) > 0:\n",
    "        extracted_contents.append(extracted_words)\n",
    "    return extracted_contents\n",
    "\n",
    "def get_Content(HTML_list):\n",
    "    html_all = []\n",
    "    for each in HTML_list:\n",
    "        each = each.strip()\n",
    "        each  = each.strip('\\n')\n",
    "        html_all.append(each)\n",
    "    str_list = ''\n",
    "    for each in html_all:\n",
    "        str_list = str_list+each\n",
    "        \n",
    "    end_sign = []\n",
    "    content = []\n",
    "    content_length = []\n",
    "    start_idx = 0\n",
    "    end_idx = 0 \n",
    "    peroid_idx = 0\n",
    "    for i in range(len(str_list)-1):\n",
    "        st = str_list[i]\n",
    "        st_next = str_list[i+1]\n",
    "        if st == '>':\n",
    "            start_idx = i\n",
    "        if st == '<' and st_next == '/':\n",
    "            end_idx = i\n",
    "        if start_idx > end_idx and end_idx>0:\n",
    "            if start_idx > end_idx + 1:\n",
    "                content.append(str_list[period_idx+1:end_idx])\n",
    "            else:\n",
    "                content.append([])\n",
    "\n",
    "            content_length.append(len(content[-1]))\n",
    "            end_sign.append(str_list[end_idx:start_idx+1])\n",
    "        period_idx = start_idx\n",
    "    \n",
    "    new_content = []\n",
    "    for i in range(len(content)):\n",
    "        if content_length[i]>0:\n",
    "            if end_sign[i] not in['</script>','</style>']:\n",
    "                new_content.append(content[i])\n",
    "    return new_content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*']\n"
     ]
    }
   ],
   "source": [
    "stopword = load_stopword('stopword.txt')\n",
    "extract_out = ['showDate','877391004','article','gpac','ori','https','◆','QQ','videoPlayerIconSpan','赞赏','投稿','原文','天前','一扫','©','tmp','周前','昨天','取消','desc']\n",
    "stopword.extend(extract_out)\n",
    "\n",
    "# lexicon = load_stopword('lexicon.txt')\n",
    "# lexicon_M = load_stopword('lexicon_by_manual.txt')\n",
    "# keyword_py = load_stopword('keyword_1_py.txt')\n",
    "# tag = load_stopword('topic.txt')\n",
    "# keyword_py.extend(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1946"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the article set: 6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import jieba\n",
    "article_list = os.listdir('txts/')\n",
    "print('length of the article set:', len(article_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "sentence_all = []\n",
    "for page in range(len(article_list)):\n",
    "    #if page==83:\n",
    "    #    continue\n",
    "    HTML_list = []\n",
    "    #contain_index_list = []\n",
    "    if page >= 0:\n",
    "        #pageid_list.append(page)\n",
    "        with open(\"txts/\"+str(article_list[page]),'r') as f:\n",
    "            for line in f:\n",
    "                HTML_list.append(line.strip())\n",
    "\n",
    "        content = HTML_list\n",
    "        extract_content = extract(content,stopword)\n",
    "        sentence = []\n",
    "        for each in extract_content:\n",
    "            if len(each) <= 1:\n",
    "                continue\n",
    "            sentence.extend(each)\n",
    "        sentence_all.append(sentence)\n",
    "        #contain_Index_list.append(contain_index_list)\n",
    "        print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['［②⑧］',\n",
       " '［②⑩］',\n",
       " '［②Ｂ］',\n",
       " '［②Ｇ］',\n",
       " '［②］',\n",
       " '［②ａ］',\n",
       " '［②ｂ］',\n",
       " '［②ｃ］',\n",
       " '［②ｄ］',\n",
       " '［②ｅ］',\n",
       " '［②ｆ］',\n",
       " '［②ｇ］',\n",
       " '［②ｈ］',\n",
       " '［②ｉ］',\n",
       " '［②ｊ］',\n",
       " '［③①］',\n",
       " '［③⑩］',\n",
       " '［③Ｆ］',\n",
       " '［③］',\n",
       " '［③ａ］',\n",
       " '［③ｂ］',\n",
       " '［③ｃ］',\n",
       " '［③ｄ］',\n",
       " '［③ｅ］',\n",
       " '［③ｇ］',\n",
       " '［③ｈ］',\n",
       " '［④］',\n",
       " '［④ａ］',\n",
       " '［④ｂ］',\n",
       " '［④ｃ］',\n",
       " '［④ｄ］',\n",
       " '［④ｅ］',\n",
       " '［⑤］',\n",
       " '［⑤］］',\n",
       " '［⑤ａ］',\n",
       " '［⑤ｂ］',\n",
       " '［⑤ｄ］',\n",
       " '［⑤ｅ］',\n",
       " '［⑤ｆ］',\n",
       " '［⑥］',\n",
       " '［⑦］',\n",
       " '［⑧］',\n",
       " '［⑨］',\n",
       " '［⑩］',\n",
       " '［＊］',\n",
       " '［－',\n",
       " '［］',\n",
       " '］',\n",
       " '］∧′＝［',\n",
       " '］［',\n",
       " '＿',\n",
       " 'ａ］',\n",
       " 'ｂ］',\n",
       " 'ｃ］',\n",
       " 'ｅ］',\n",
       " 'ｆ］',\n",
       " 'ｎｇ昉',\n",
       " '｛',\n",
       " '｛－',\n",
       " '｜',\n",
       " '｝',\n",
       " '｝＞',\n",
       " '～',\n",
       " '～±',\n",
       " '～＋',\n",
       " '￥',\n",
       " 'the',\n",
       " 'and',\n",
       " 'of',\n",
       " 'or',\n",
       " 'to',\n",
       " 'a',\n",
       " 'in',\n",
       " 'T',\n",
       " 'P',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " 'e',\n",
       " 'fi',\n",
       " 'showDate',\n",
       " '877391004',\n",
       " 'article',\n",
       " 'gpac',\n",
       " 'ori',\n",
       " 'https',\n",
       " '◆',\n",
       " 'QQ',\n",
       " 'videoPlayerIconSpan',\n",
       " '赞赏',\n",
       " '投稿',\n",
       " '原文',\n",
       " '天前',\n",
       " '一扫',\n",
       " '©',\n",
       " 'tmp',\n",
       " '周前',\n",
       " '昨天',\n",
       " '取消',\n",
       " 'desc']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#构造文章的用词矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentence_all = extract(content,stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for sentence in sentence_all:\n",
    "    for w in sentence:\n",
    "        vocab.append(w)\n",
    "vocab = list(set(vocab))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5037\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def convert2vec_with_weights_scampus(content, vocab, lexicon, tag, kw_weights = 1.0, not_kw_weights = 1.0, tag_weights = 1.0):\n",
    "    '''\n",
    "    针对关键词与非关键词采用不同权重的One-hot编码转换成vector\n",
    "    :param content:\n",
    "    :param vocab: 所有词词典\n",
    "    :param lexicon: 关键词词表\n",
    "    :param kw_weights:\n",
    "    :param not_kw_weights:\n",
    "    :return:\n",
    "    '''\n",
    "    vec = []\n",
    "    for each in content:\n",
    "        each_vec = [0] * len(vocab)\n",
    "        for w in each:\n",
    "            if w in lexicon:\n",
    "                each_vec[vocab.index(w)] += kw_weights\n",
    "            else:\n",
    "                if w in tag:\n",
    "                    each_vec[vocab.index(w)] += tag_weights\n",
    "                else:\n",
    "                    each_vec[vocab.index(w)] += not_kw_weights\n",
    "        vec.append(each_vec)\n",
    "    return vec\n",
    "\n",
    "def convert2vec_with_weights(content, vocab, kw_weights = 1.0, not_kw_weights = 1.0, tag_weights = 1.0):\n",
    "    '''\n",
    "    针对关键词与非关键词采用不同权重的One-hot编码转换成vector\n",
    "    :param content:\n",
    "    :param vocab: 所有词词典\n",
    "    :param lexicon: 关键词词表\n",
    "    :param kw_weights:\n",
    "    :param not_kw_weights:\n",
    "    :return:\n",
    "    '''\n",
    "    vec = []\n",
    "    for each in content:\n",
    "        each_vec = [0] * len(vocab)\n",
    "        for w in each:\n",
    "            each_vec[vocab.index(w)] += not_kw_weights\n",
    "        vec.append(each_vec)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataX = np.array(convert2vec_with_weights(sentence_all, vocab, 1, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5037\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reasonably'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)\n",
      "[[0.00643882 0.00381989 0.         ... 0.00321941 0.         0.00321941]\n",
      " [0.         0.00622777 0.00286938 ... 0.         0.00242253 0.        ]\n",
      " [0.         0.00877795 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.00369887 0.        ]\n",
      " [0.         0.00622777 0.00286938 ... 0.         0.00242253 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "import scipy\n",
    "X = scipy.sparse.csr_matrix(dataX)\n",
    "#类调用  \n",
    "transformer = TfidfTransformer()  \n",
    "print( transformer)  \n",
    "#将词频矩阵X统计成TF-IDF值  \n",
    "tfidf = transformer.fit_transform(X)  \n",
    "#查看数据结构 tfidf[i][j]表示i类文本中的tf-idf权重  \n",
    "print( tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:\n",
      "['τ', 'Sinkhorn', 'we', 'cid', 'is', 'as', 'ﬁ', 'S', 'that', 'with']\n",
      "title:\n",
      "['edge', 'for', 'model', 'is', 'we', 'with', 'on', 'i', 'decoder', 'x']\n",
      "title:\n",
      "['ﬀ', 'is', 'discs', 'Dense', 'disc', 'that', 'di', 'network', 'erent', 'this']\n",
      "title:\n",
      "['The', 'objects', 'relations', 'as', 'with', 'object', 'ﬁ', 'that', 'were', 'model']\n",
      "title:\n",
      "['edge', 'for', 'model', 'is', 'we', 'with', 'on', 'i', 'decoder', 'x']\n",
      "title:\n",
      "['aims', 'diver', 'walking', 'middle', 'vs', 'McGregor', 'angles', 'H', 'states', 'place']\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(sentence_all)):\n",
    "    sort_array = np.argsort(tfidf.toarray()[j])\n",
    "    keywords_list = [vocab[sort_array[-(i+1)]] for i in range(10)]\n",
    "    print('title:')\n",
    "    print(keywords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)\n"
     ]
    }
   ],
   "source": [
    "transformer = TfidfTransformer()  \n",
    "print( transformer)  \n",
    "#将词频矩阵X统计成TF-IDF值  \n",
    "tfidf = transformer.fit_transform(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.18143287 ... 0.00445768 0.         0.00643882]\n",
      " [0.00286938 0.00286938 0.10577089 ... 0.01453518 0.         0.        ]\n",
      " [0.         0.         0.14781896 ... 0.         0.00739807 0.        ]\n",
      " [0.         0.         0.14781122 ... 0.         0.         0.        ]\n",
      " [0.00286938 0.00286938 0.10577089 ... 0.01453518 0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print( tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.sparse.csr_matrix(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
